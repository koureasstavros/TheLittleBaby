{
	"c_sequence": "pre",
    "c_attention": "mha",
    "c_network": "mlp",
    "n_ctx": 128,
    "n_emb": 128,
    "dropout": 0.1,
    "head_size": 128,
    "n_heads": 16,
    "n_layers": 16,
    "num_epochs": 1,
    "batch_size": 16,
    "lr": 0.001
}