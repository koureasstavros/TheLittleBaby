{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë∂ The Little Baby\n",
    "\n",
    "> A barebones GPT-style LLM implementation ‚Äî pure Python, zero dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4frr4zkTZ0i"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from pathlib import Path as pt\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Model Workflow\n",
    "########################\n",
    "\n",
    "model_load_workflow = input(\"Enter the model load workflow (train/inference/extract): \").strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_guid(guid_str):\n",
    "    try:\n",
    "        val = uuid.UUID(guid_str)\n",
    "        return str(val) == guid_str  # Ensures exact format match\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Hyper‚Äêparameters\n",
    "########################\n",
    "\n",
    "if model_load_workflow == \"train\":\n",
    "    # Generate a UUID (random UUID)\n",
    "    model_config_uuid = uuid.uuid4()\n",
    "    config = {\n",
    "        # model parameters \n",
    "        \"n_ctx\": 128,          # used for chunking in the training phase and for positional embeddings in the inference phase | x2 the time of batch & size of the model\n",
    "        \"n_emb\": 128,         # embedding dimension for each token which is for each character | x2 the time of batch & size of the model\n",
    "        \"dropout\": 0.1,       # dropout probability\n",
    "        \"head_size\": 128,     # total projection dim (to be split into heads) | x2 the time of batch from a certain point & size of the model\n",
    "        \"n_heads\": 16,        # number of attention heads\n",
    "        \"n_layers\": 4,       # number of transformer layers | x2 the time of batch from a certain point & size of the model\n",
    "\n",
    "        # training parameters\n",
    "        \"num_epochs\": 1,      # number of epochs to train\n",
    "        \"batch_size\": 16,     # batch size of words to train | x2 the time of batch\n",
    "        \"lr\": 1e-3            # learning rate\n",
    "    }\n",
    "elif model_load_workflow == \"inference\" or model_load_workflow == \"extract\":\n",
    "    # Provide the UUID or Last\n",
    "    model_config_uuid_inpt = input(\"Enter the model configuration (<uuid>/last): \").lower()\n",
    "\n",
    "    # Load the last UUID and Prompt\n",
    "    if model_config_uuid_inpt == \"last\":\n",
    "        with open(f'config/last.json', 'r') as f:\n",
    "            last = json.load(f)\n",
    "            model_config_uuid_last = last['last_uuid']            \n",
    "            prompt_last = last['last_prompt']\n",
    "            model_config_uuid = model_config_uuid_last\n",
    "    else:\n",
    "        model_config_uuid = model_config_uuid_inpt\n",
    "    \n",
    "    # Load the configuration from a file\n",
    "    if is_valid_guid(model_config_uuid):\n",
    "        with open(f'outputs/report_{model_config_uuid}.json', 'r') as f:\n",
    "            report = json.load(f)\n",
    "            config = report['config']\n",
    "    else:\n",
    "        print(f\"Invalid UUID: {model_config_uuid}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Model GUID\n",
    "########################\n",
    "\n",
    "print(f\"Model GUID: {model_config_uuid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvOsJSS3S0dD"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Tokenization\n",
    "########################\n",
    "\n",
    "# Create a dummy input.txt for demonstration purposes\n",
    "# In a real scenario, this file would be provided by the user.\n",
    "try:\n",
    "    with open('inputs/input.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(\"inputs/input.txt not found. Creating a dummy file for demonstration.\")\n",
    "    dummy_text = \"This is a sample text for training a tiny GPT model. It contains various characters and will be used to demonstrate the backpropagation implementation. The quick brown fox jumps over the lazy dog. This text is long enough to create some batches for training. We hope this works! Backpropagation is fundamental to training neural networks. It allows us to compute gradients efficiently.\"\n",
    "    with open('input.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(dummy_text)\n",
    "    text = dummy_text # Use the dummy text directly\n",
    "\n",
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "itos = {i: c for i, c in enumerate(vocab)}  # int-to-string\n",
    "stoi = {c: i for i, c in enumerate(vocab)}  # string-to-int\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "data = encode(text)\n",
    "split = int(0.9 * len(data))\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]\n",
    "\n",
    "########################\n",
    "# Data Preparation\n",
    "########################\n",
    "\n",
    "def prepare_data(data, n_ctx):\n",
    "    X, y = [], []\n",
    "    # Ensure there's enough data for at least one full context length + 1 for target\n",
    "    if len(data) < n_ctx + 1:\n",
    "        # Return empty arrays with correct shapes for concatenation later\n",
    "        return np.array([], dtype=np.int32).reshape(0, n_ctx), np.array([], dtype=np.int32).reshape(0, n_ctx)\n",
    "\n",
    "    for i in range(0, len(data) - n_ctx):\n",
    "        X.append(data[i:i+n_ctx])\n",
    "        y.append(data[i+1:i+n_ctx+1])\n",
    "    return np.array(X, dtype=np.int32), np.array(y, dtype=np.int32)\n",
    "\n",
    "X_train, y_train = prepare_data(train_data, config[\"n_ctx\"])\n",
    "X_val, y_val = prepare_data(val_data, config[\"n_ctx\"])\n",
    "\n",
    "def get_batches(X, y, b_size, shuffle=True):\n",
    "    N = X.shape[0]\n",
    "    if N == 0: # Handle empty data case\n",
    "        return # Yield nothing if no data\n",
    "    indices = np.arange(N)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for i in range(0, N, b_size):\n",
    "        batch_idx = indices[i:i+b_size]\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAtiHplETkKY"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Base Module \n",
    "########################\n",
    "\n",
    "class Module:\n",
    "    \"\"\"\n",
    "    Base class for all neural network modules.\n",
    "    Manages parameters and training/evaluation mode.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.training = True # Default mode for modules\n",
    "        self._parameters = [] # List to store trainable parameters\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list of all trainable parameters in the module.\n",
    "        This method should be overridden by subclasses.\n",
    "        \"\"\"\n",
    "        return self._parameters\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"\n",
    "        Sets the module and all its sub-modules to training mode.\n",
    "        If mode is False, sets to evaluation mode.\n",
    "        \"\"\"\n",
    "        self.training = mode\n",
    "        # Recursively set training mode for any sub-modules\n",
    "        for param in self._parameters: # Iterate through parameters, which might be other Modules\n",
    "            if isinstance(param, Module):\n",
    "                param.train(mode)\n",
    "            # For actual numpy arrays (weights/biases), the `training` flag is used by Dropout\n",
    "            # and other layers that behave differently in train/eval.\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Sets the module to evaluation mode.\"\"\"\n",
    "        self.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Helper Functions\n",
    "########################\n",
    "\n",
    "# GELU activation and its derivative\n",
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit (GELU) activation function.\"\"\"\n",
    "    return 0.5 * x * (1.0 + np.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "\n",
    "def gelu_prime(x):\n",
    "    \"\"\"Derivative of the GELU activation function.\"\"\"\n",
    "    # This is an approximation of the derivative, common in practice.\n",
    "    # The exact derivative involves erf, which is complex.\n",
    "    # This simplified version uses the derivative of tanh.\n",
    "    k = math.sqrt(2/math.pi) * (x + 0.044715 * np.power(x, 3))\n",
    "    sech_sq = 1 / np.cosh(k)**2 # sech^2(x) = 1 / cosh^2(x)\n",
    "    k_prime = math.sqrt(2/math.pi) * (1 + 3 * 0.044715 * np.power(x, 2))\n",
    "    return 0.5 * (1 + np.tanh(k)) + 0.5 * x * sech_sq * k_prime\n",
    "\n",
    "# Softmax (along given axis)\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Computes softmax probabilities along a given axis for numerical stability.\"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    e_x = np.exp(x - x_max)\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfY9loUtT3Zs"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Layers\n",
    "########################\n",
    "\n",
    "class Embedding(Module):\n",
    "    \"\"\"\n",
    "    A simple Embedding layer.\n",
    "    Maps integer indices to dense vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Initialize weights with small random values\n",
    "        self.weight = np.random.randn(num_embeddings, embedding_dim) * 0.02\n",
    "        self._parameters = [self.weight] # Register weight as a parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for Embedding layer.\n",
    "        x: input indices (e.g., token IDs), shape (B, T) or (T,)\n",
    "        Returns: embedded vectors, shape (B, T, embedding_dim) or (T, embedding_dim)\n",
    "        \"\"\"\n",
    "        self._cache = x # Store input indices for backward pass\n",
    "        return self.weight[x]\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for Embedding layer.\n",
    "        grad_output: gradient from the subsequent layer, shape (B, T, embedding_dim)\n",
    "        Returns: (grad_input, [grad_weight])\n",
    "        \"\"\"\n",
    "        x = self._cache # Retrieve input indices\n",
    "        grad_weight = np.zeros_like(self.weight) # Initialize gradient for weights\n",
    "\n",
    "        # Accumulate gradients for each embedding used.\n",
    "        # This is a sparse update: only update rows corresponding to input indices.\n",
    "        if x.ndim == 1: # Handle (T,) input case\n",
    "            for i, idx in enumerate(x):\n",
    "                grad_weight[idx] += grad_output[i]\n",
    "        else: # Handle (B, T) input case\n",
    "            for b in range(x.shape[0]):\n",
    "                for t in range(x.shape[1]):\n",
    "                    grad_weight[x[b, t]] += grad_output[b, t]\n",
    "\n",
    "        # For embedding layer, there's no gradient to pass back to the input (it's just indices).\n",
    "        return None, [grad_weight]\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A simple Linear (fully connected) layer.\n",
    "    Performs y = x @ W + b.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        # Initialize weights with small random values\n",
    "        self.weight = np.random.randn(in_features, out_features) * 0.02\n",
    "        self.bias = np.zeros(out_features) if bias else None\n",
    "        self._parameters = [self.weight]\n",
    "        if self.bias is not None:\n",
    "            self._parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for Linear layer.\n",
    "        x: input tensor, shape (..., in_features)\n",
    "        Returns: output tensor, shape (..., out_features)\n",
    "        \"\"\"\n",
    "        self._cache = x # Store input for backward pass\n",
    "        out = x.dot(self.weight)\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for Linear layer.\n",
    "        grad_output: gradient from the subsequent layer, shape (..., out_features)\n",
    "        Returns: (grad_input, [grad_weight, grad_bias])\n",
    "        \"\"\"\n",
    "        x = self._cache # Retrieve input\n",
    "        original_x_shape = x.shape\n",
    "\n",
    "        # Reshape x and grad_output to 2D for matrix multiplication for gradients\n",
    "        # This handles arbitrary leading dimensions (B, T, ...)\n",
    "        x_reshaped = x.reshape(-1, original_x_shape[-1]) # (N, in_features)\n",
    "        grad_output_reshaped = grad_output.reshape(-1, grad_output.shape[-1]) # (N, out_features)\n",
    "\n",
    "        # Gradient for weight: dL/dW = x.T @ dL/dy\n",
    "        grad_weight = x_reshaped.T @ grad_output_reshaped\n",
    "\n",
    "        grad_bias = None\n",
    "        if self.bias is not None:\n",
    "            # Gradient for bias: dL/db = sum(dL/dy) along all dimensions except the last\n",
    "            grad_bias = np.sum(grad_output_reshaped, axis=0)\n",
    "\n",
    "        # Gradient for input: dL/dx = dL/dy @ W.T\n",
    "        grad_input = grad_output_reshaped @ self.weight.T\n",
    "        grad_input = grad_input.reshape(original_x_shape) # Reshape back to original input shape\n",
    "\n",
    "        param_grads = [grad_weight]\n",
    "        if grad_bias is not None:\n",
    "            param_grads.append(grad_bias)\n",
    "\n",
    "        return grad_input, param_grads\n",
    "\n",
    "class LayerNorm(Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization layer.\n",
    "    Normalizes features across the last dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, dims, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = np.ones(dims)  # Learnable scaling parameter\n",
    "        self.beta = np.zeros(dims)  # Learnable shifting parameter\n",
    "        self._parameters = [self.gamma, self.beta]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for LayerNorm.\n",
    "        x: input tensor, shape (..., dims)\n",
    "        Returns: normalized tensor, same shape as x\n",
    "        \"\"\"\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        var = x.var(axis=-1, keepdims=True)\n",
    "        std = np.sqrt(var + self.eps)\n",
    "        x_norm = (x - mean) / std # Normalized input\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "\n",
    "        # Store intermediate values for backward pass\n",
    "        self._cache = (x, mean, std, x_norm)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for LayerNorm.\n",
    "        grad_output: gradient from subsequent layer, same shape as forward output.\n",
    "        Returns: (grad_input, [grad_gamma, grad_beta])\n",
    "        \"\"\"\n",
    "        x, mean, std, x_norm = self._cache\n",
    "\n",
    "        # Gradients for gamma and beta (sum over all dimensions except the last)\n",
    "        grad_gamma = np.sum(grad_output * x_norm, axis=tuple(range(grad_output.ndim - 1)))\n",
    "        grad_beta = np.sum(grad_output, axis=tuple(range(grad_output.ndim - 1)))\n",
    "\n",
    "        # Gradient for x_norm (before gamma/beta scaling)\n",
    "        grad_x_norm = grad_output * self.gamma\n",
    "\n",
    "        # Gradient for x (through normalization formula)\n",
    "        # This is a common and numerically stable way to compute LayerNorm's grad_x\n",
    "        N = x.shape[-1] # Number of features in the last dimension\n",
    "\n",
    "        # Part 1: Gradient through (x - mean) / std\n",
    "        grad_x = grad_x_norm / std\n",
    "\n",
    "        # Part 2: Gradient through std (which comes from var)\n",
    "        grad_var = np.sum(grad_x_norm * (x - mean) * (-0.5) * (std**(-3)), axis=-1, keepdims=True)\n",
    "\n",
    "        # Part 3: Gradient through mean\n",
    "        grad_mean = np.sum(grad_x_norm * (-1 / std), axis=-1, keepdims=True) + grad_var * (-2 / N) * np.sum(x - mean, axis=-1, keepdims=True)\n",
    "\n",
    "        grad_x += (2 / N) * grad_var * (x - mean)\n",
    "        grad_x += (1 / N) * grad_mean\n",
    "\n",
    "        return grad_x, [grad_gamma, grad_beta]\n",
    "\n",
    "class Dropout(Module):\n",
    "    \"\"\"\n",
    "    Dropout layer.\n",
    "    Randomly sets a fraction of input units to zero during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p # Dropout probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for Dropout.\n",
    "        x: input tensor\n",
    "        Returns: output tensor with dropout applied (if training)\n",
    "        \"\"\"\n",
    "        if self.training and self.p > 0:\n",
    "            # Create a mask: True for elements to keep, False for elements to drop\n",
    "            # Scale by 1/(1-p) during training (inverted dropout)\n",
    "            mask = (np.random.rand(*x.shape) >= self.p) / (1.0 - self.p)\n",
    "            self._cache = mask # Store mask for backward pass\n",
    "            return x * mask\n",
    "        self._cache = None # No mask if not training or p=0\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for Dropout.\n",
    "        grad_output: gradient from subsequent layer.\n",
    "        Returns: (grad_input, []) - no parameters to update.\n",
    "        \"\"\"\n",
    "        if self._cache is not None: # Only apply mask if dropout was active in forward\n",
    "            mask = self._cache\n",
    "            grad_input = grad_output * mask\n",
    "        else: # If dropout was not active, simply pass gradient through\n",
    "            grad_input = grad_output\n",
    "        return grad_input, [] # Dropout has no parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7IompsKTqYX"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Attention Mechanism\n",
    "########################\n",
    "\n",
    "class MultiHeadAttention(Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention mechanism.\n",
    "    Computes attention scores and combines information from different \"heads\".\n",
    "    \"\"\"\n",
    "    def __init__(self, n_emb, head_size, n_heads, n_ctx, dropout_p):\n",
    "        super().__init__()\n",
    "        self.n_emb = n_emb\n",
    "        self.head_size = head_size  # total projection dim (e.g. 128)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = head_size // n_heads  # dimension per head (e.g. 32)\n",
    "\n",
    "        # Linear projections for Query, Key, Value\n",
    "        self.q_proj = Linear(n_emb, head_size, bias=False)\n",
    "        self.k_proj = Linear(n_emb, head_size, bias=False)\n",
    "        self.v_proj = Linear(n_emb, head_size, bias=False)\n",
    "\n",
    "        # Output projection\n",
    "        self.c_proj = Linear(head_size, n_emb)\n",
    "\n",
    "        self.attn_dropout = Dropout(dropout_p)\n",
    "        self.resid_dropout = Dropout(dropout_p)\n",
    "\n",
    "        # Causal mask to prevent looking ahead in sequence (for decoder-only models)\n",
    "        causal_mask = np.triu(np.ones((n_ctx, n_ctx)) * -1e9, k=1)\n",
    "        self.causal_mask = causal_mask\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns all parameters of the attention module.\"\"\"\n",
    "        return (self.q_proj.parameters() +\n",
    "                self.k_proj.parameters() +\n",
    "                self.v_proj.parameters() +\n",
    "                self.c_proj.parameters())\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"Sets the attention module and its sub-modules to training/eval mode.\"\"\"\n",
    "        super().train(mode) # Call base Module train to set self.training\n",
    "        self.q_proj.train(mode)\n",
    "        self.k_proj.train(mode)\n",
    "        self.v_proj.train(mode)\n",
    "        self.c_proj.train(mode)\n",
    "        self.attn_dropout.train(mode)\n",
    "        self.resid_dropout.train(mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for MultiHeadAttention.\n",
    "        x: input tensor, shape (B, T, n_emb)\n",
    "        Returns: output tensor, shape (B, T, n_emb)\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # Project input to Q, K, V\n",
    "        Q_orig = self.q_proj.forward(x)  # (B, T, head_size)\n",
    "        K_orig = self.k_proj.forward(x)\n",
    "        V_orig = self.v_proj.forward(x)\n",
    "\n",
    "        # Helper function to split heads and transpose\n",
    "        def split_heads(z):\n",
    "            B_s, T_s, H_s = z.shape\n",
    "            z = z.reshape(B_s, T_s, self.n_heads, self.d_k)\n",
    "            return z.transpose(0, 2, 1, 3) # (B, n_heads, T, d_k)\n",
    "\n",
    "        Q = split_heads(Q_orig)\n",
    "        K = split_heads(K_orig)\n",
    "        V = split_heads(V_orig)\n",
    "\n",
    "        # Compute scaled dot-product attention scores\n",
    "        # (B, n_heads, T, d_k) @ (B, n_heads, d_k, T) -> (B, n_heads, T, T)\n",
    "        scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply causal mask (prevents attending to future tokens)\n",
    "        masked_scores = scores + self.causal_mask[:T, :T]\n",
    "\n",
    "        attn_weights = softmax(masked_scores, axis=-1)\n",
    "        attn_weights_dropped = self.attn_dropout.forward(attn_weights)\n",
    "\n",
    "        # Compute weighted sum of values\n",
    "        # (B, n_heads, T, T) @ (B, n_heads, T, d_k) -> (B, n_heads, T, d_k)\n",
    "        o = np.matmul(attn_weights_dropped, V)\n",
    "\n",
    "        # Recombine heads: transpose and reshape back to (B, T, head_size)\n",
    "        o_combined = o.transpose(0, 2, 1, 3).reshape(B, T, self.head_size)\n",
    "\n",
    "        # Final linear projection\n",
    "        out = self.c_proj.forward(o_combined)\n",
    "        out_dropped = self.resid_dropout.forward(out)\n",
    "\n",
    "        # Store all intermediate values needed for backward pass\n",
    "        self._cache = (x, Q_orig, K_orig, V_orig, Q, K, V, scores, masked_scores, attn_weights, attn_weights_dropped, o, o_combined)\n",
    "        return out_dropped\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for MultiHeadAttention.\n",
    "        grad_output: gradient from the subsequent layer, shape (B, T, n_emb)\n",
    "        Returns: (grad_input, list_of_param_grads)\n",
    "        \"\"\"\n",
    "        (x, Q_orig, K_orig, V_orig, Q, K, V, scores, masked_scores, attn_weights, attn_weights_dropped, o, o_combined) = self._cache\n",
    "\n",
    "        # Gradients will be collected in the order of self.parameters(): q_proj, k_proj, v_proj, c_proj\n",
    "        current_mha_param_grads = []\n",
    "\n",
    "        # 1. Backward through resid_dropout\n",
    "        grad_out_dropped, _ = self.resid_dropout.backward(grad_output) # Dropout has no params\n",
    "\n",
    "        # 2. Backward through c_proj (output linear layer)\n",
    "        grad_o_combined, c_proj_grads = self.c_proj.backward(grad_out_dropped)\n",
    "\n",
    "        # 3. Undo reshape/transpose for o_combined to get grad_o\n",
    "        # grad_o_combined: (B, T, head_size)\n",
    "        # grad_o: (B, n_heads, T, d_k)\n",
    "        B, T, H = grad_o_combined.shape\n",
    "        grad_o = grad_o_combined.reshape(B, T, self.n_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "\n",
    "        # 4. Backward through matmul(attn_weights_dropped, V)\n",
    "        # o = A @ V  => dL/dA = dL/do @ V.T, dL/dV = A.T @ dL/do\n",
    "        grad_attn_weights_dropped = np.matmul(grad_o, V.transpose(0, 1, 3, 2))\n",
    "        grad_V = np.matmul(attn_weights_dropped.transpose(0, 1, 3, 2), grad_o)\n",
    "\n",
    "        # 5. Backward through attn_dropout\n",
    "        grad_attn_weights, _ = self.attn_dropout.backward(grad_attn_weights_dropped)\n",
    "\n",
    "        # 6. Backward through softmax (attn_weights = softmax(masked_scores))\n",
    "        # dL/dx = y * (dL/dy - sum(dL/dy * y)) where y = softmax(x)\n",
    "        grad_masked_scores = grad_attn_weights * attn_weights - np.sum(grad_attn_weights * attn_weights, axis=-1, keepdims=True) * attn_weights\n",
    "\n",
    "        # 7. Backward through scores + causal_mask (causal_mask is constant, so its gradient is 0)\n",
    "        grad_scores = grad_masked_scores\n",
    "\n",
    "        # 8. Backward through scaled dot-product: scores = (Q @ K.T) / sqrt(d_k)\n",
    "        # Let S = Q @ K.T / sqrt(d_k)\n",
    "        # dL/dQ = (dL/dS @ K) / sqrt(d_k)\n",
    "        # dL/dK = (dL/dS.T @ Q) / sqrt(d_k)\n",
    "        grad_Q = np.matmul(grad_scores, K) / math.sqrt(self.d_k)\n",
    "        grad_K = np.matmul(grad_scores.transpose(0, 1, 3, 2), Q) / math.sqrt(self.d_k)\n",
    "\n",
    "        # 9. Undo split_heads for Q, K, V to get gradients for original Q_orig, K_orig, V_orig\n",
    "        def un_split_heads(z_grad, original_shape):\n",
    "            B_s, NH_s, T_s, DK_s = z_grad.shape\n",
    "            z_grad = z_grad.transpose(0, 2, 1, 3) # (B, T, n_heads, d_k)\n",
    "            return z_grad.reshape(original_shape) # (B, T, head_size)\n",
    "\n",
    "        grad_Q_orig = un_split_heads(grad_Q, Q_orig.shape)\n",
    "        grad_K_orig = un_split_heads(grad_K, K_orig.shape)\n",
    "        grad_V_orig = un_split_heads(grad_V, V_orig.shape)\n",
    "\n",
    "        # 10. Backward through q_proj, k_proj, v_proj\n",
    "        grad_x_q, q_proj_grads = self.q_proj.backward(grad_Q_orig)\n",
    "        grad_x_k, k_proj_grads = self.k_proj.backward(grad_K_orig)\n",
    "        grad_x_v, v_proj_grads = self.v_proj.backward(grad_V_orig)\n",
    "\n",
    "        # Sum gradients for the input 'x' from all three paths (Q, K, V)\n",
    "        grad_x = grad_x_q + grad_x_k + grad_x_v\n",
    "\n",
    "        # Assemble gradients in the correct order: q_proj, k_proj, v_proj, c_proj\n",
    "        current_mha_param_grads.extend(q_proj_grads)\n",
    "        current_mha_param_grads.extend(k_proj_grads)\n",
    "        current_mha_param_grads.extend(v_proj_grads)\n",
    "        current_mha_param_grads.extend(c_proj_grads)\n",
    "\n",
    "        return grad_x, current_mha_param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4jWRdI3TILl"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Multi-Layer Perceptron (MLP)\n",
    "########################\n",
    "\n",
    "class MLP(Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron block, typically used in Transformer after attention.\n",
    "    Consists of two linear layers with GELU activation and dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_emb, dropout_p):\n",
    "        super().__init__()\n",
    "        # First linear layer (expands dimension)\n",
    "        self.c_fc = Linear(n_emb, 4 * n_emb)\n",
    "        # Second linear layer (projects back to original dimension)\n",
    "        self.c_proj = Linear(4 * n_emb, n_emb)\n",
    "        self.dropout = Dropout(dropout_p)\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns all parameters of the MLP module.\"\"\"\n",
    "        return self.c_fc.parameters() + self.c_proj.parameters()\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"Sets the MLP module and its sub-modules to training/eval mode.\"\"\"\n",
    "        super().train(mode)\n",
    "        self.c_fc.train(mode)\n",
    "        self.c_proj.train(mode)\n",
    "        self.dropout.train(mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for MLP.\n",
    "        x: input tensor, shape (B, T, n_emb)\n",
    "        Returns: output tensor, shape (B, T, n_emb)\n",
    "        \"\"\"\n",
    "        self._cache_x = x # Store input to MLP for backward pass\n",
    "\n",
    "        fc_out = self.c_fc.forward(x)\n",
    "        gelu_out = gelu(fc_out)\n",
    "        proj_out = self.c_proj.forward(gelu_out)\n",
    "        dropped_out = self.dropout.forward(proj_out)\n",
    "\n",
    "        # Store intermediate results for backward pass\n",
    "        self._cache = (fc_out, gelu_out, proj_out)\n",
    "        return dropped_out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for MLP.\n",
    "        grad_output: gradient from subsequent layer.\n",
    "        Returns: (grad_input, list_of_param_grads)\n",
    "        \"\"\"\n",
    "        x = self._cache_x\n",
    "        fc_out, gelu_out, proj_out = self._cache\n",
    "\n",
    "        # Gradients will be collected in the order of self.parameters(): c_fc, c_proj\n",
    "        current_mlp_param_grads = []\n",
    "\n",
    "        # 1. Backward through dropout\n",
    "        grad_proj_out, _ = self.dropout.backward(grad_output)\n",
    "\n",
    "        # 2. Backward through c_proj\n",
    "        grad_gelu_out, c_proj_grads = self.c_proj.backward(grad_proj_out)\n",
    "\n",
    "\n",
    "        # 3. Backward through GELU activation\n",
    "        # dL/dx = dL/dy * gelu_prime(x)\n",
    "        grad_fc_out = grad_gelu_out * gelu_prime(fc_out)\n",
    "\n",
    "        # 4. Backward through c_fc\n",
    "        grad_x, c_fc_grads = self.c_fc.backward(grad_fc_out)\n",
    "\n",
    "        # Assemble gradients in the correct order: c_fc, c_proj\n",
    "        current_mlp_param_grads.extend(c_fc_grads)\n",
    "        current_mlp_param_grads.extend(c_proj_grads)\n",
    "\n",
    "        return grad_x, current_mlp_param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTTLhgQxTRXL"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Transformer Block\n",
    "########################\n",
    "\n",
    "class Block(Module):\n",
    "    \"\"\"\n",
    "    A single Transformer Block.\n",
    "    Consists of LayerNorm, MultiHeadAttention, another LayerNorm, and an MLP.\n",
    "    Includes residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_emb, head_size, n_heads, n_ctx, dropout_p):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(n_emb)\n",
    "        self.mha = MultiHeadAttention(n_emb, head_size, n_heads, n_ctx, dropout_p)\n",
    "        self.ln_2 = LayerNorm(n_emb)\n",
    "        self.mlp = MLP(n_emb, dropout_p)\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns all parameters of the Transformer Block.\"\"\"\n",
    "        return (self.ln_1.parameters() +\n",
    "                self.mha.parameters() + # Order changed for consistency\n",
    "                self.ln_2.parameters() +\n",
    "                self.mlp.parameters())\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"Sets the block and its sub-modules to training/eval mode.\"\"\"\n",
    "        super().train(mode)\n",
    "        self.ln_1.train(mode)\n",
    "        self.ln_2.train(mode)\n",
    "        self.mha.train(mode)\n",
    "        self.mlp.train(mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for a Transformer Block.\n",
    "        x: input tensor, shape (B, T, n_emb)\n",
    "        Returns: output tensor, shape (B, T, n_emb)\n",
    "        \"\"\"\n",
    "        self._cache_x = x # Store input for the first residual connection\n",
    "\n",
    "        # First residual connection: x + MHA(LayerNorm(x))\n",
    "        ln1_out = self.ln_1.forward(x)\n",
    "        mha_out = self.mha.forward(ln1_out)\n",
    "        x_res1 = x + mha_out # Residual connection 1\n",
    "\n",
    "        # Second residual connection: x_res1 + MLP(LayerNorm(x_res1))\n",
    "        ln2_out = self.ln_2.forward(x_res1)\n",
    "        mlp_out = self.mlp.forward(ln2_out)\n",
    "        out = x_res1 + mlp_out # Residual connection 2\n",
    "\n",
    "        # Store intermediate values for backward pass\n",
    "        self._cache = (ln1_out, mha_out, ln2_out, mlp_out, x_res1)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for a Transformer Block.\n",
    "        grad_output: gradient from subsequent layer.\n",
    "        Returns: (grad_input, list_of_param_grads)\n",
    "        \"\"\"\n",
    "        x = self._cache_x\n",
    "        ln1_out, mha_out, ln2_out, mlp_out, x_res1 = self._cache\n",
    "\n",
    "        # Gradients will be collected in the order of self.parameters(): ln_1, mha, ln_2, mlp\n",
    "        current_block_param_grads = []\n",
    "\n",
    "        # 1. Backward through second residual connection and MLP\n",
    "        grad_x_res1_from_res2 = grad_output\n",
    "        grad_mlp_out = grad_output\n",
    "\n",
    "        grad_ln2_out, mlp_grads = self.mlp.backward(grad_mlp_out)\n",
    "\n",
    "        grad_x_res1_from_ln2, ln2_grads = self.ln_2.backward(grad_ln2_out)\n",
    "\n",
    "        # Sum gradients for x_res1 from both paths\n",
    "        grad_x_res1 = grad_x_res1_from_res2 + grad_x_res1_from_ln2\n",
    "\n",
    "        # 2. Backward through first residual connection and MHA\n",
    "        grad_x_from_res1 = grad_x_res1\n",
    "        grad_mha_out = grad_x_res1\n",
    "\n",
    "        grad_ln1_out, mha_grads = self.mha.backward(grad_mha_out)\n",
    "\n",
    "        grad_x_from_ln1, ln1_grads = self.ln_1.backward(grad_ln1_out)\n",
    "\n",
    "        # Sum gradients for the initial input 'x' from both paths\n",
    "        grad_x = grad_x_from_res1 + grad_x_from_ln1\n",
    "\n",
    "        # Assemble gradients in the correct order: ln_1, mha, ln_2, mlp\n",
    "        current_block_param_grads.extend(ln1_grads)\n",
    "        current_block_param_grads.extend(mha_grads)\n",
    "        current_block_param_grads.extend(ln2_grads)\n",
    "        current_block_param_grads.extend(mlp_grads)\n",
    "\n",
    "        return grad_x, current_block_param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3U5D8UeTR7r"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# GPT Model Definition\n",
    "########################\n",
    "\n",
    "class GPT(Module):\n",
    "    \"\"\"\n",
    "    A minimal GPT (Generative Pre-trained Transformer) model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, n_ctx, n_emb, n_layers, head_size, n_heads, dropout_p):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.wte = Embedding(vocab_size, n_emb)    # Token embeddings\n",
    "        self.wpe = Embedding(n_ctx, n_emb)        # Positional embeddings\n",
    "        self.blocks = [Block(n_emb, head_size, n_heads, n_ctx, dropout_p)\n",
    "                       for _ in range(n_layers)]    # Stack of Transformer blocks\n",
    "        self.ln_f = LayerNorm(n_emb)                # Final Layer Normalization\n",
    "        self.lm_head = Linear(n_emb, vocab_size)    # Language modeling head (output logits)\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns all parameters of the GPT model.\"\"\"\n",
    "        params = []\n",
    "        params += self.wte.parameters()\n",
    "        params += self.wpe.parameters()\n",
    "        for block in self.blocks:\n",
    "            params += block.parameters()\n",
    "        params += self.ln_f.parameters()\n",
    "        params += self.lm_head.parameters()\n",
    "        return params\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"Sets the GPT model and all its sub-modules to training/eval mode.\"\"\"\n",
    "        super().train(mode) # Call base Module train to set self.training\n",
    "        self.wte.train(mode)\n",
    "        self.wpe.train(mode)\n",
    "        for block in self.blocks:\n",
    "            block.train(mode)\n",
    "        self.ln_f.train(mode)\n",
    "        self.lm_head.train(mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the GPT model.\n",
    "        x: input token IDs, shape (B, T)\n",
    "        Returns: logits, shape (B, T, vocab_size)\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.wte.forward(x)           # (B, T, n_emb)\n",
    "        pos_indices = np.arange(T)              # Positional indices for current sequence length\n",
    "        pos_emb = self.wpe.forward(pos_indices) # (T, n_emb)\n",
    "\n",
    "        # Combine token and position embeddings (positional embeddings are broadcasted)\n",
    "        x_combined_emb = tok_emb + pos_emb\n",
    "\n",
    "        # Pass through Transformer blocks\n",
    "        current_x = x_combined_emb\n",
    "        # We need to store the output of each block to correctly backpropagate through the sequential blocks.\n",
    "        # However, the Block's backward method only needs its *own* input gradient, not the full history.\n",
    "        # The chain rule handles this sequentially.\n",
    "        for block in self.blocks:\n",
    "            current_x = block.forward(current_x)\n",
    "\n",
    "        ln_f_out = self.ln_f.forward(current_x)\n",
    "        logits = self.lm_head.forward(ln_f_out)  # (B, T, vocab_size)\n",
    "\n",
    "        # Store intermediate values for backward pass\n",
    "        self._cache = (x_combined_emb, current_x, ln_f_out)\n",
    "        return logits\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for the GPT model.\n",
    "        grad_output: gradient from the loss function, shape (B, T, vocab_size)\n",
    "        Returns: (None, list_of_param_grads) - no grad_input for the whole model.\n",
    "        \"\"\"\n",
    "        (x_combined_emb, current_x_before_lnf, ln_f_out) = self._cache\n",
    "\n",
    "        # Initialize an empty list to store gradients in the correct order\n",
    "        # The order must match self.parameters()\n",
    "        ordered_param_grads = []\n",
    "\n",
    "        # 1. Backward through lm_head\n",
    "        grad_ln_f_out, lm_head_grads = self.lm_head.backward(grad_output)\n",
    "\n",
    "        # 2. Backward through ln_f (final LayerNorm)\n",
    "        grad_current_x_before_lnf, ln_f_grads = self.ln_f.backward(grad_ln_f_out)\n",
    "\n",
    "        # 3. Backward through blocks in reverse order\n",
    "        # Need to store block gradients temporarily in correct order (forward pass order)\n",
    "        # to match how self.blocks are added in parameters()\n",
    "        block_grads_temp = [None] * len(self.blocks) # Temporary storage for block gradients\n",
    "        grad_for_prev_block = grad_current_x_before_lnf\n",
    "        for i in reversed(range(len(self.blocks))):\n",
    "            block = self.blocks[i]\n",
    "            grad_for_prev_block, current_block_grads = block.backward(grad_for_prev_block)\n",
    "            block_grads_temp[i] = current_block_grads # Store in forward order index\n",
    "\n",
    "        # 4. Backward through token + position embeddings addition\n",
    "        # grad_for_prev_block is now the gradient for x_combined_emb (tok_emb + pos_emb)\n",
    "        grad_tok_emb = grad_for_prev_block # Gradient for token embeddings\n",
    "        # For position embeddings, sum gradients over the batch dimension\n",
    "        grad_pos_emb = np.sum(grad_for_prev_block, axis=0)\n",
    "\n",
    "        # 5. Backward through wte (token embeddings) and wpe (position embeddings)\n",
    "        # Embedding.backward returns (None, [grad_weight])\n",
    "        _, wte_grads = self.wte.backward(grad_tok_emb)\n",
    "        _, wpe_grads = self.wpe.backward(grad_pos_emb)\n",
    "\n",
    "        # Now, assemble the ordered_param_grads list in the same order as self.parameters()\n",
    "        ordered_param_grads.extend(wte_grads)\n",
    "        ordered_param_grads.extend(wpe_grads)\n",
    "        for grads_list_for_block in block_grads_temp: # These are already in forward order\n",
    "            ordered_param_grads.extend(grads_list_for_block)\n",
    "        ordered_param_grads.extend(ln_f_grads)\n",
    "        ordered_param_grads.extend(lm_head_grads)\n",
    "\n",
    "        return None, ordered_param_grads # No grad_input for the entire model\n",
    "\n",
    "    def generate(self, prompt, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generates new tokens based on the model's learned probabilities.\n",
    "        prompt_ids: input sequence of token IDs to start generation.\n",
    "        max_new_tokens: maximum number of tokens to generate.\n",
    "        Returns: generated sequence of token IDs.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode for generation (disables dropout)\n",
    "        self.eval()\n",
    "        # Start with a starting token (here we use index 0, assuming it's a valid token)\n",
    "        # This could be a special <SOS> token in a more robust implementation.\n",
    "        if prompt is None:\n",
    "            ctx = np.zeros((1, 1), dtype=np.int32) # Initial context: a single token\n",
    "        else:\n",
    "            encode_ = lambda s: np.array([stoi[c] for c in s]).reshape(1, -1)\n",
    "            prompt_ids = encode_(prompt)  # Your tokenizer should return shape (1, prompt_length)\n",
    "            ctx = prompt_ids # Initial context: sequence of tokens\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Use the last self.n_ctx tokens as input (or all if shorter)\n",
    "            # This handles the fixed context window of the model\n",
    "            input_seq = ctx[:, -self.n_ctx:]\n",
    "\n",
    "            logits = self.forward(input_seq)\n",
    "\n",
    "            # Get logits for the last token in the sequence (the one to predict)\n",
    "            logits = logits[:, -1, :] # Shape: (1, vocab_size)\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probs = softmax(logits, axis=-1).flatten() # Flatten to (vocab_size,)\n",
    "\n",
    "            # Sample the next token based on probabilities\n",
    "            next_tok = np.random.choice(np.arange(probs.shape[0]), p=probs)\n",
    "\n",
    "            # Append the new token to the context\n",
    "            ctx = np.concatenate([ctx, np.array([[next_tok]], dtype=np.int32)], axis=1)\n",
    "\n",
    "        # Set model back to training mode after generation\n",
    "        self.train()\n",
    "        return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwZhSm2QUmj7"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Loss Function\n",
    "########################\n",
    "\n",
    "def cross_entropy_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss and its gradient with respect to logits.\n",
    "    logits: (B, T, vocab_size) - raw predictions from the model\n",
    "    targets: (B, T) - true token IDs\n",
    "    Returns: (loss_value, grad_logits)\n",
    "    \"\"\"\n",
    "    B, T, C = logits.shape\n",
    "    logits_flat = logits.reshape(B * T, C)\n",
    "    targets_flat = targets.reshape(B * T)\n",
    "\n",
    "    # For numerical stability: subtract max logit from all logits before exponentiation\n",
    "    logits_max = np.max(logits_flat, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits_flat - logits_max)\n",
    "\n",
    "    sum_exp_logits = np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    probs = exp_logits / sum_exp_logits # Softmax probabilities\n",
    "\n",
    "    # Compute loss: - sum(target_one_hot * log(probs))\n",
    "    log_probs = np.log(probs + 1e-9) # Add epsilon for numerical stability to avoid log(0)\n",
    "    loss = -np.mean(log_probs[np.arange(B * T), targets_flat])\n",
    "\n",
    "    # Compute gradient of cross-entropy loss with respect to logits\n",
    "    # The derivative of Cross-Entropy + Softmax is (probs - one_hot_targets)\n",
    "    one_hot_targets = np.zeros_like(probs)\n",
    "    one_hot_targets[np.arange(B * T), targets_flat] = 1\n",
    "\n",
    "    grad_logits = probs - one_hot_targets # Shape: (B*T, C)\n",
    "    grad_logits = grad_logits.reshape(B, T, C) # Reshape back to (B, T, C)\n",
    "\n",
    "    return loss, grad_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fLVEmUnUtAC"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Calculate Value and Grand\n",
    "########################\n",
    "\n",
    "def value_and_grad(model, x, y):\n",
    "    \"\"\"\n",
    "    Performs a forward pass to compute loss and then a backward pass\n",
    "    to compute gradients for all model parameters.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    logits = model.forward(x)\n",
    "    # Compute loss and get initial gradient for logits from the loss function\n",
    "    loss, grad_logits = cross_entropy_loss(logits, y)\n",
    "\n",
    "    # Backward pass: The model's backward method takes the gradient from the loss\n",
    "    # and propagates it back through all layers, returning gradients for parameters.\n",
    "    _, grads = model.backward(grad_logits) # grad_input for model is None\n",
    "\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f__OAAbdTB3n"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Optimizer and Training Loop\n",
    "########################\n",
    "\n",
    "class AdamW:\n",
    "    \"\"\"\n",
    "    AdamW optimizer implementation.\n",
    "    Includes adaptive learning rates and weight decay.\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters, learning_rate=1e-3, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01):\n",
    "        self.params = parameters # List of all trainable parameters (NumPy arrays)\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.t = 0 # Timestep counter\n",
    "\n",
    "        # Initialize first and second moment estimates for each parameter\n",
    "        self.m = {id(p): np.zeros_like(p) for p in self.params}\n",
    "        self.v = {id(p): np.zeros_like(p) for p in self.params}\n",
    "\n",
    "    def step(self, grads):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step (parameter update).\n",
    "        grads: a list of gradients, corresponding to self.params.\n",
    "        \"\"\"\n",
    "        self.t += 1 # Increment timestep\n",
    "        for p, g in zip(self.params, grads):\n",
    "            pid = id(p) # Use object ID for unique parameter identification\n",
    "\n",
    "            # Apply weight decay (L2 regularization)\n",
    "            # This is applied directly to the gradient before the Adam update\n",
    "            g = g + self.weight_decay * p\n",
    "\n",
    "            # Update biased first moment estimate\n",
    "            self.m[pid] = self.beta1 * self.m[pid] + (1 - self.beta1) * g\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[pid] = self.beta2 * self.v[pid] + (1 - self.beta2) * (g * g)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = self.m[pid] / (1 - self.beta1 ** self.t)\n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            v_hat = self.v[pid] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update parameters\n",
    "            p -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AG0rezctS98o"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = GPT(vocab_size, config[\"n_ctx\"], config[\"n_emb\"], config[\"n_layers\"], config[\"head_size\"], config[\"n_heads\"], config[\"dropout\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0oI0zhVS72_"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Training Loop\n",
    "########################\n",
    "\n",
    "if model_load_workflow == \"train\":\n",
    "    params = model.parameters() # Get all trainable parameters from the model\n",
    "    optimizer = AdamW(params, learning_rate=config[\"lr\"])\n",
    "\n",
    "    batch_logs = []\n",
    "    epoch_logs = []\n",
    "    epoch_total_time = 0\n",
    "\n",
    "    print(\"--- Start training ---\")\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        epoch_start_time = time.time()  # Record start time\n",
    "\n",
    "        # Training phase\n",
    "        model.train(True) # Set model to training mode (enables dropout)\n",
    "        running_train_loss = 0\n",
    "        train_batch_cnt = 0\n",
    "        train_batch_all = 0\n",
    "\n",
    "        train_batches = list(get_batches(X_train, y_train, config[\"batch_size\"], shuffle=True))\n",
    "        train_batch_all = len(train_batches)\n",
    "        if not train_batches:\n",
    "            print(f\"Epoch {epoch:2} over {config[\"num_epochs\"]:2} | No training data batches available. Skipping training for this epoch.\")\n",
    "            avg_train_loss = float('nan')\n",
    "        else:\n",
    "            for X_batch, y_batch in train_batches:\n",
    "                train_batch_cnt += 1\n",
    "                train_batch_start_time = time.time()  # Record start time\n",
    "                # Compute loss and gradients\n",
    "                loss, grads = value_and_grad(model, X_batch, y_batch)\n",
    "                # Update model parameters using the optimizer\n",
    "                optimizer.step(grads)\n",
    "                running_train_loss += loss\n",
    "                train_batch_stop_time = time.time()  # Record stop time\n",
    "                train_batch_elapsed_time = train_batch_stop_time - train_batch_start_time\n",
    "                # Print loss for the current batch\n",
    "                batch_log = f\"Batch {train_batch_cnt:2} over {train_batch_all:2} | train_loss = {loss:.4f} | execution_time = {train_batch_elapsed_time:.4f}\"\n",
    "                batch_logs.append(batch_log)\n",
    "                print(batch_log)\n",
    "            avg_train_loss = running_train_loss / train_batch_cnt\n",
    "\n",
    "        # Validation phase\n",
    "        model.train(False) # Set model to evaluation mode (disables dropout)\n",
    "        running_val_loss = 0\n",
    "        val_batch_cnt = 0\n",
    "        val_batch_all = 0\n",
    "\n",
    "        val_batches = list(get_batches(X_val, y_val, config[\"batch_size\"], shuffle=False))\n",
    "        val_batch_all = len(val_batches)\n",
    "        if not val_batches:\n",
    "            print(f\"Epoch {(epoch+1):2} over {config[\"num_epochs\"]:2} | No validation data batches available. Skipping validation for this epoch.\")\n",
    "            avg_val_loss = float('nan') # Indicate no validation was performed\n",
    "        else:\n",
    "            for X_batch, y_batch in val_batches:\n",
    "                val_batch_cnt += 1\n",
    "                val_batch_start_time = time.time()  # Record start time\n",
    "                # In validation, only forward pass and loss computation are needed\n",
    "                logits = model.forward(X_batch)\n",
    "                loss, _ = cross_entropy_loss(logits, y_batch) # Don't need gradients for validation\n",
    "                running_val_loss += loss\n",
    "                val_batch_stop_time = time.time()  # Record stop time\n",
    "                val_batch_elapsed_time = val_batch_stop_time - val_batch_start_time  \n",
    "                # Print loss for the current batch\n",
    "                batch_log = f\"Batch {val_batch_cnt:2} over {val_batch_all:2} | val_loss = {loss:.4f} | execution_time = {val_batch_elapsed_time:.4f}\"\n",
    "                batch_logs.append(batch_log)\n",
    "                print(batch_log)\n",
    "            avg_val_loss = running_val_loss / val_batch_cnt\n",
    "        \n",
    "        epoch_stop_time = time.time()  # Record stop time\n",
    "        epoch_elapsed_time = epoch_stop_time - epoch_start_time  \n",
    "        epoch_total_time += epoch_elapsed_time\n",
    "\n",
    "        # Print average losses for the epoch\n",
    "        epoch_log = f\"Epoch {(epoch+1):2} over {config[\"num_epochs\"]:2} | train_loss = {avg_train_loss:.4f} | val_loss = {avg_val_loss:.4f} | execution_time = {epoch_elapsed_time:.4f}\"\n",
    "        epoch_logs.append(epoch_log)\n",
    "        print(epoch_log)\n",
    "\n",
    "    epoch_total_time_avg = epoch_total_time / config[\"num_epochs\"]\n",
    "    print(f\"Average epoch time: {epoch_total_time_avg:.4f}\")\n",
    "    \n",
    "    print(\"--- Stop training ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Save the report\n",
    "########################\n",
    "\n",
    "if model_load_workflow == \"train\":\n",
    "    analysis = {\n",
    "        \"num_epochs\": config[\"num_epochs\"],\n",
    "        \"train_batches_per_epoch\": train_batch_all,\n",
    "        \"val_batches_per_epoch\": val_batch_all,\n",
    "        \"average_train_loss_per_epoch\": avg_train_loss,\n",
    "        \"average_val_loss_per_epoch\": avg_val_loss,\n",
    "        \"average_train_time_per_epoch\": epoch_total_time_avg\n",
    "    }\n",
    "    \n",
    "    report = {\n",
    "        \"config\": config,\n",
    "        \"analysis\": analysis,\n",
    "        \"batch_logs\": batch_logs,\n",
    "        \"epoch_logs\": epoch_logs\n",
    "    }\n",
    "    \n",
    "    with open(f\"outputs/report_{model_config_uuid}.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=4) # indent for pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Save the model\n",
    "########################\n",
    "\n",
    "if model_load_workflow == \"train\":\n",
    "    #Determine how to save the model based on the model_load_method\n",
    "    model_load_method = \"both\"  # \"both\" or \"object\" or \"weights\"\n",
    "\n",
    "    if model_load_method == \"object\" or model_load_method == \"both\":\n",
    "        # This method stores the entire model into a single file\n",
    "        with open(f'models/model_{model_config_uuid}_all.pkl', 'wb') as f:\n",
    "            pk.dump(model, f)\n",
    "    if model_load_method == \"weights\" or model_load_method == \"both\":        \n",
    "        # Extract all weight arrays from the model\n",
    "        weights_dict = {\n",
    "            # Token and position embeddings\n",
    "            'wte_weight': model.wte.weight,\n",
    "            'wpe_weight': model.wpe.weight,\n",
    "            \n",
    "            # Final layer norm\n",
    "            'ln_f_gamma': model.ln_f.gamma,\n",
    "            'ln_f_beta': model.ln_f.beta,\n",
    "            \n",
    "            # Language model head\n",
    "            'lm_head_weight': model.lm_head.weight,\n",
    "            'lm_head_bias': model.lm_head.bias if model.lm_head.bias is not None else None,\n",
    "        }\n",
    "        \n",
    "        # Add block weights\n",
    "        for i, block in enumerate(model.blocks):\n",
    "            # Layer norms\n",
    "            weights_dict[f'block_{i}_ln1_gamma'] = block.ln_1.gamma\n",
    "            weights_dict[f'block_{i}_ln1_beta'] = block.ln_1.beta\n",
    "            weights_dict[f'block_{i}_ln2_gamma'] = block.ln_2.gamma\n",
    "            weights_dict[f'block_{i}_ln2_beta'] = block.ln_2.beta\n",
    "            \n",
    "            # Multi-head attention\n",
    "            weights_dict[f'block_{i}_mha_q_weight'] = block.mha.q_proj.weight\n",
    "            weights_dict[f'block_{i}_mha_k_weight'] = block.mha.k_proj.weight\n",
    "            weights_dict[f'block_{i}_mha_v_weight'] = block.mha.v_proj.weight\n",
    "            weights_dict[f'block_{i}_mha_c_weight'] = block.mha.c_proj.weight\n",
    "            weights_dict[f'block_{i}_mha_c_bias'] = block.mha.c_proj.bias if block.mha.c_proj.bias is not None else None\n",
    "            \n",
    "            # MLP\n",
    "            weights_dict[f'block_{i}_mlp_fc_weight'] = block.mlp.c_fc.weight\n",
    "            weights_dict[f'block_{i}_mlp_fc_bias'] = block.mlp.c_fc.bias if block.mlp.c_fc.bias is not None else None\n",
    "            weights_dict[f'block_{i}_mlp_proj_weight'] = block.mlp.c_proj.weight\n",
    "            weights_dict[f'block_{i}_mlp_proj_bias'] = block.mlp.c_proj.bias if block.mlp.c_proj.bias is not None else None\n",
    "        \n",
    "        # Convert numpy arrays to lists for JSON serialization\n",
    "        json_weights_dict = {}\n",
    "        for key, value in weights_dict.items():\n",
    "            if value is not None and hasattr(value, 'tolist'):\n",
    "                json_weights_dict[key] = value.tolist()\n",
    "            else:\n",
    "                json_weights_dict[key] = value\n",
    "\n",
    "        # Save weights only\n",
    "        with open(f'models/{model_config_uuid}.weights', 'w') as f:\n",
    "            json.dump(json_weights_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Load the model\n",
    "########################\n",
    "\n",
    "if model_load_workflow == \"train\" or model_load_workflow == \"inference\":\n",
    "    #Determine how to load the model based on the model_load_method\n",
    "    model_load_method = \"weights\" # \"both\" or \"object\" or \"weights\"\n",
    "\n",
    "    if model_load_method == \"object\" or model_load_method == \"both\":\n",
    "        # This method loads the entire model from a single file\n",
    "        with open(f'models/model_{model_config_uuid}.pkl', 'rb') as f:\n",
    "            model = pk.load(f)\n",
    "    if model_load_method == \"weights\" or model_load_method == \"both\":\n",
    "        with open(f'models/model_{model_config_uuid}.weights', 'r') as f:\n",
    "            json_weights_dict = json.load(f)\n",
    "\n",
    "        # Convert lists back to numpy arrays\n",
    "        weights_dict = {}\n",
    "        for key, value in json_weights_dict.items():\n",
    "            if value is not None and isinstance(value, list):\n",
    "                weights_dict[key] = np.array(value, dtype=np.float32)\n",
    "            else:\n",
    "                weights_dict[key] = value\n",
    "            \n",
    "        # Restore weights to the model\n",
    "        model.wte.weight = weights_dict['wte_weight']\n",
    "        model.wpe.weight = weights_dict['wpe_weight']\n",
    "        model.ln_f.gamma = weights_dict['ln_f_gamma']\n",
    "        model.ln_f.beta = weights_dict['ln_f_beta']\n",
    "        model.lm_head.weight = weights_dict['lm_head_weight']\n",
    "        if weights_dict['lm_head_bias'] is not None:\n",
    "            model.lm_head.bias = weights_dict['lm_head_bias']\n",
    "        \n",
    "        # Restore block weights\n",
    "        for i, block in enumerate(model.blocks):\n",
    "            # Layer norms\n",
    "            block.ln_1.gamma = weights_dict[f'block_{i}_ln1_gamma']\n",
    "            block.ln_1.beta = weights_dict[f'block_{i}_ln1_beta']\n",
    "            block.ln_2.gamma = weights_dict[f'block_{i}_ln2_gamma']\n",
    "            block.ln_2.beta = weights_dict[f'block_{i}_ln2_beta']\n",
    "            \n",
    "            # Multi-head attention\n",
    "            block.mha.q_proj.weight = weights_dict[f'block_{i}_mha_q_weight']\n",
    "            block.mha.k_proj.weight = weights_dict[f'block_{i}_mha_k_weight']\n",
    "            block.mha.v_proj.weight = weights_dict[f'block_{i}_mha_v_weight']\n",
    "            block.mha.c_proj.weight = weights_dict[f'block_{i}_mha_c_weight']\n",
    "            if weights_dict[f'block_{i}_mha_c_bias'] is not None:\n",
    "                block.mha.c_proj.bias = weights_dict[f'block_{i}_mha_c_bias']\n",
    "            \n",
    "            # MLP\n",
    "            block.mlp.c_fc.weight = weights_dict[f'block_{i}_mlp_fc_weight']\n",
    "            if weights_dict[f'block_{i}_mlp_fc_bias'] is not None:\n",
    "                block.mlp.c_fc.bias = weights_dict[f'block_{i}_mlp_fc_bias']\n",
    "            block.mlp.c_proj.weight = weights_dict[f'block_{i}_mlp_proj_weight']\n",
    "            if weights_dict[f'block_{i}_mlp_proj_bias'] is not None:\n",
    "                block.mlp.c_proj.bias = weights_dict[f'block_{i}_mlp_proj_bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Extract model components\n",
    "########################\n",
    "\n",
    "if model_load_workflow == \"extract\":    \n",
    "    model_load_workflow_type = input(\"Enter the extract type (from_pkl_obj/from_pkl_wgt): \").lower()\n",
    "    \n",
    "    model_config_uuids = []\n",
    "    if model_config_uuid == \"all\":\n",
    "        # Define the directory\n",
    "        directory = pt('models')\n",
    "\n",
    "        # Regex pattern for GUID\n",
    "        pattern = re.compile(r'model_([a-f0-9\\-]{36})\\.pkl')\n",
    "\n",
    "        # Find matching files and extract GUIDs\n",
    "        for file in directory.glob('model_*.pkl'):\n",
    "            match = pattern.match(file.name)\n",
    "            if match:\n",
    "                model_config_uuids.append(match.group(1))\n",
    "\n",
    "    # Loop through each GUID to extract weights\n",
    "    for model_config_uuid in model_config_uuids:\n",
    "        if model_load_workflow_type == \"from_pkl_obj\":\n",
    "            # Load the model from a single file\n",
    "            with open(f'models/model_{model_config_uuid}.pkl', 'rb') as f:\n",
    "                model_ = pk.load(f)\n",
    "\n",
    "            # Extract all weight arrays from the model\n",
    "            weights_dict = {\n",
    "                # Token and position embeddings\n",
    "                'wte_weight': model_.wte.weight,\n",
    "                'wpe_weight': model_.wpe.weight,\n",
    "                \n",
    "                # Final layer norm\n",
    "                'ln_f_gamma': model_.ln_f.gamma,\n",
    "                'ln_f_beta': model_.ln_f.beta,\n",
    "                \n",
    "                # Language model head\n",
    "                'lm_head_weight': model_.lm_head.weight,\n",
    "                'lm_head_bias': model_.lm_head.bias if model_.lm_head.bias is not None else None,\n",
    "            }\n",
    "\n",
    "            # Add block weights\n",
    "            for i, block in enumerate(model_.blocks):\n",
    "                # Layer norms\n",
    "                weights_dict[f'block_{i}_ln1_gamma'] = block.ln_1.gamma\n",
    "                weights_dict[f'block_{i}_ln1_beta'] = block.ln_1.beta\n",
    "                weights_dict[f'block_{i}_ln2_gamma'] = block.ln_2.gamma\n",
    "                weights_dict[f'block_{i}_ln2_beta'] = block.ln_2.beta\n",
    "                \n",
    "                # Multi-head attention\n",
    "                weights_dict[f'block_{i}_mha_q_weight'] = block.mha.q_proj.weight\n",
    "                weights_dict[f'block_{i}_mha_k_weight'] = block.mha.k_proj.weight\n",
    "                weights_dict[f'block_{i}_mha_v_weight'] = block.mha.v_proj.weight\n",
    "                weights_dict[f'block_{i}_mha_c_weight'] = block.mha.c_proj.weight\n",
    "                weights_dict[f'block_{i}_mha_c_bias'] = block.mha.c_proj.bias if block.mha.c_proj.bias is not None else None\n",
    "                \n",
    "                # MLP\n",
    "                weights_dict[f'block_{i}_mlp_fc_weight'] = block.mlp.c_fc.weight\n",
    "                weights_dict[f'block_{i}_mlp_fc_bias'] = block.mlp.c_fc.bias if block.mlp.c_fc.bias is not None else None\n",
    "                weights_dict[f'block_{i}_mlp_proj_weight'] = block.mlp.c_proj.weight\n",
    "                weights_dict[f'block_{i}_mlp_proj_bias'] = block.mlp.c_proj.bias if block.mlp.c_proj.bias is not None else None\n",
    "\n",
    "            # Store the model weights to a single file\n",
    "            with open(f'models/model_{model_config_uuid}.weights', 'w') as f:\n",
    "                json.dump(weights_dict, f, indent=4)\n",
    "\n",
    "        elif model_load_workflow_type == \"from_pkl_wgt\":\n",
    "            with open(f'models/model_{model_config_uuid}.pkl', 'rb') as f:\n",
    "                weights_dict = pk.load(f)\n",
    "\n",
    "            # Convert numpy arrays to lists for JSON serialization\n",
    "            json_weights_dict = {}\n",
    "            for key, value in weights_dict.items():\n",
    "                if value is not None and hasattr(value, 'tolist'):\n",
    "                    json_weights_dict[key] = value.tolist()\n",
    "                else:\n",
    "                    json_weights_dict[key] = value\n",
    "\n",
    "            # Store the model weights to a single file\n",
    "            with open(f'models/model_{model_config_uuid}.weights', 'w') as f:\n",
    "                json.dump(json_weights_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIzRAyRMS5bo"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Inference (Generation)\n",
    "########################\n",
    "\n",
    "inference_total_time = 0\n",
    "if model_load_workflow == \"train\" or model_load_workflow == \"inference\":\n",
    "    print(\"--- Start Inference ---\")\n",
    "    \n",
    "    # Generate 500 new tokens\n",
    "    inference_max_tokens = 500\n",
    "\n",
    "    if model_load_workflow == \"train\":\n",
    "        prompt = None\n",
    "        completion_output_name = f\"outputs/completion_{model_config_uuid}.json\"\n",
    "    elif model_load_workflow == \"inference\":\n",
    "        prompt_inpt = input(\"Enter the prompt for inference (<prompt>/last/none): \").lower()\n",
    "        if prompt_inpt == \"last\":\n",
    "            prompt = prompt_last\n",
    "        elif prompt_inpt == \"none\":\n",
    "            prompt = None\n",
    "        else:\n",
    "            prompt = prompt_inpt\n",
    "        \n",
    "        today = dt.today()\n",
    "        today_ft = today.strftime('%Y%m%d%H%M%S')\n",
    "        completion_output_name = f\"outputs/completion_{model_config_uuid}_{today_ft}.json\"\n",
    "    \n",
    "    inference_start_time = time.time()  # Record start time\n",
    "\n",
    "    # Generate text based on given token ids\n",
    "    generation_ids = model.generate(prompt, inference_max_tokens)\n",
    "    \n",
    "    # Decode the generated token IDs back to text\n",
    "    generation = decode(generation_ids[0].tolist())\n",
    "\n",
    "    inference_stop_time = time.time()  # Record stop time\n",
    "    inference_elapsed_time = inference_stop_time - inference_start_time  \n",
    "    inference_total_time += inference_elapsed_time\n",
    "\n",
    "    # Create a completion object with the prompt, generated text, and inference time\n",
    "    completion = { \"prompt\": prompt, \"generation\": generation, \"inference_max_tokens\": inference_max_tokens, \"inference_total_time\": inference_total_time }\n",
    "\n",
    "    print(completion)\n",
    "    print(\"--- Stop Inference ---\")\n",
    "\n",
    "    # Save the completion text to a file\n",
    "    with open(completion_output_name, 'w', encoding='utf-8') as f:\n",
    "        json.dump(completion, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the last settings text to a file\n",
    "if is_valid_guid(model_config_uuid):\n",
    "    last = { \"last_uuid\": str(model_config_uuid), \"last_prompt\": str(prompt) }\n",
    "    with open('config/last.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(last, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0s/R8uzSuCuXm8ZakpFkh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python-3-12-9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
