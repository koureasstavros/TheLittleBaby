{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘¶ The Little Baby\n",
    "\n",
    "> A barebones GPT-style LLM implementation â€” pure Python, zero dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cores = os.cpu_count() or 8\n",
    "print(f\"Detected CPU cores: {cores}\")\n",
    "if \"OMP_NUM_THREADS\" not in os.environ:\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(cores)\n",
    "if \"MKL_NUM_THREADS\" not in os.environ:\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(cores)\n",
    "if \"OPENBLAS_NUM_THREADS\" not in os.environ:\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(cores)\n",
    "if \"NUMEXPR_NUM_THREADS\" not in os.environ:\n",
    "    os.environ[\"NUMEXPR_NUM_THREADS\"] = str(cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4frr4zkTZ0i"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Runtime helpers\n",
    "########################\n",
    "\n",
    "def is_valid_guid(guid_str):\n",
    "    try:\n",
    "        val = uuid.UUID(guid_str)\n",
    "        return str(val) == guid_str  # Ensures exact format match\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def from_file(file_path, file_mode):\n",
    "    try:\n",
    "        if file_mode == \"plain\":\n",
    "            with open(file_path, \"r\") as f:\n",
    "                file = f.read()\n",
    "                return file\n",
    "        elif file_mode == \"json\":\n",
    "            with open(file_path, \"r\") as f:\n",
    "                file = json.load(f)\n",
    "                return file\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        exit(1)\n",
    "        \n",
    "\n",
    "def to_file(file_path, file_mode, content):\n",
    "    try:\n",
    "        if file_mode == \"plain\":\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(content)\n",
    "        elif file_mode == \"json\":\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(content, f, indent=4)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Tokenizer Class\n",
    "########################\n",
    "\n",
    "class CharTokenizer:\n",
    "    \"\"\"\n",
    "    Character-level tokenizer for text processing.\n",
    "    Separates encoding/decoding logic for better modularity.\n",
    "    \"\"\"\n",
    "    def __init__(self, text=None, vocab=None, stoi=None, itos=None):\n",
    "        self.vocab = None\n",
    "        self.vocab_size = None\n",
    "        self.itos = None\n",
    "        self.stoi = None\n",
    "\n",
    "    def init(self, text):\n",
    "        self.vocab = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.itos = {i: c for i, c in enumerate(self.vocab)}\n",
    "        self.stoi = {c: i for i, c in enumerate(self.vocab)}\n",
    "\n",
    "    def expand_vocabilary(self, text):\n",
    "        \"\"\" Expand the tokenizer vocabulary with new characters from the text. \"\"\"\n",
    "        new_chars = set(text) - set(self.vocab)\n",
    "        if new_chars:\n",
    "            print(f\"Found {len(new_chars)} new characters: {new_chars}\")\n",
    "            # Add new characters to existing vocabulary and sort everything\n",
    "            all_chars = set(self.vocab) | new_chars  # Union of sets\n",
    "            self.vocab = sorted(list(all_chars))\n",
    "            self.vocab_size = len(self.vocab)\n",
    "            self.itos = {i: c for i, c in enumerate(self.vocab)}\n",
    "            self.stoi = {c: i for i, c in enumerate(self.vocab)}\n",
    "\n",
    "    def fit(self, text):\n",
    "        \"\"\" Fit the tokenizer to the provided text. \"\"\"\n",
    "        if self.vocab is None:\n",
    "            self.init(text)\n",
    "        else:\n",
    "            self.expand_vocabilary(text)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\" Convert string to list of token IDs. \"\"\"        \n",
    "        return [self.stoi[c] for c in text]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\" Convert list of token IDs to string. \"\"\"\n",
    "        return ''.join([self.itos[i] for i in token_ids])\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\" Export tokenizer state as dictionary for JSON serialization. \"\"\"\n",
    "        return {\n",
    "            'vocab': self.vocab,\n",
    "            'vocab_size': self.vocab_size\n",
    "        }\n",
    "    \n",
    "    def from_dict(self, tokenizer_dict):\n",
    "        \"\"\" Load tokenizer from dictionary. \"\"\"\n",
    "        self.vocab=tokenizer_dict['vocab']\n",
    "        self.vocab_size=len(self.vocab)\n",
    "        self.itos={int(k): v for k, v in enumerate(self.vocab)} # Convert keys back to integers\n",
    "        self.stoi={str(v): k for k, v in enumerate(self.vocab)} # Convert keys back to strings\n",
    "\n",
    "    def tokenize(self, input_path):\n",
    "        \"\"\" Tokenize input text and return data. \"\"\"\n",
    "        text = from_file(input_path, \"plain\")\n",
    "        \n",
    "        # Fit the tokenizer to the text\n",
    "        self.fit(text)\n",
    "\n",
    "        # Encode the text\n",
    "        data = self.encode(text)\n",
    "        split = int(0.9 * len(data))\n",
    "        train_data = data[:split]\n",
    "        val_data = data[split:]\n",
    "\n",
    "        return train_data, val_data\n",
    "    \n",
    "    def prepare_data(self, data, n_ctx):\n",
    "        \"\"\" Prepares data for training by creating input-output pairs. \"\"\"\n",
    "\n",
    "        X, y = [], []\n",
    "        # Ensure there's enough data for at least one full context length + 1 for target\n",
    "        if len(data) < n_ctx + 1:\n",
    "            # Return empty arrays with correct shapes for concatenation later\n",
    "            return np.array([], dtype=np.int32).reshape(0, n_ctx), np.array([], dtype=np.int32).reshape(0, n_ctx)\n",
    "\n",
    "        for i in range(0, len(data) - n_ctx):\n",
    "            X.append(data[i:i+n_ctx])\n",
    "            y.append(data[i+1:i+n_ctx+1])\n",
    "        return np.array(X, dtype=np.int32), np.array(y, dtype=np.int32)\n",
    "    \n",
    "    def get_batches(self, X, y, b_size, shuffle=True):\n",
    "        \"\"\" Generates batches of data for training. \"\"\"\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        if N == 0: # Handle empty data case\n",
    "            return # Yield nothing if no data\n",
    "        indices = np.arange(N)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for i in range(0, N, b_size):\n",
    "            batch_idx = indices[i:i+b_size]\n",
    "            yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAtiHplETkKY"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Base Module \n",
    "########################\n",
    "\n",
    "class Module:\n",
    "    \"\"\"\n",
    "    Base class for all neural network modules.\n",
    "    Manages parameters and training/evaluation mode.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.setting = True # Default mode for modules\n",
    "        self._parameters = [] # List to store trainable parameters\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list of all trainable parameters in the module.\n",
    "        This method should be overridden by subclasses.\n",
    "        \"\"\"\n",
    "        return self._parameters\n",
    "\n",
    "    def set(self, mode=True):\n",
    "        \"\"\"\n",
    "        Sets the module and all its sub-modules to training mode.\n",
    "        If mode is False, sets to evaluation mode.\n",
    "        \"\"\"\n",
    "        self.setting = mode\n",
    "        # Recursively set training mode for any sub-modules\n",
    "        for param in self._parameters: # Iterate through parameters, which might be other Modules\n",
    "            if isinstance(param, Module):\n",
    "                param.set(mode)\n",
    "            # For actual numpy arrays (weights/biases), the `training` flag is used by Dropout\n",
    "            # and other layers that behave differently in train/eval.\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Sets the module to evaluation mode.\"\"\"\n",
    "        self.set(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Helper Functions\n",
    "########################\n",
    "\n",
    "# GELU activation and its derivative\n",
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit (GELU) activation function.\"\"\"\n",
    "    return 0.5 * x * (1.0 + np.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "\n",
    "def gelu_prime(x):\n",
    "    \"\"\"Derivative of the GELU activation function.\"\"\"\n",
    "    # This is an approximation of the derivative, common in practice.\n",
    "    # The exact derivative involves erf, which is complex.\n",
    "    # This simplified version uses the derivative of tanh.\n",
    "    k = math.sqrt(2/math.pi) * (x + 0.044715 * np.power(x, 3))\n",
    "    sech_sq = 1 / np.cosh(k)**2 # sech^2(x) = 1 / cosh^2(x)\n",
    "    k_prime = math.sqrt(2/math.pi) * (1 + 3 * 0.044715 * np.power(x, 2))\n",
    "    return 0.5 * (1 + np.tanh(k)) + 0.5 * x * sech_sq * k_prime\n",
    "\n",
    "# Softmax (along given axis)\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Computes softmax probabilities along a given axis for numerical stability.\"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    e_x = np.exp(x - x_max)\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfY9loUtT3Zs"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Layers\n",
    "########################\n",
    "\n",
    "class Embedding(Module):\n",
    "    \"\"\"\n",
    "    A simple Embedding layer.\n",
    "    Maps integer indices to dense vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Initialize weights with small random values\n",
    "        self.weight = np.random.randn(num_embeddings, embedding_dim) * 0.02\n",
    "        self._parameters = [self.weight] # Register weight as a parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for Embedding layer.\n",
    "        x: input indices (e.g., token IDs), shape (B, T) or (T,)\n",
    "        Returns: embedded vectors, shape (B, T, embedding_dim) or (T, embedding_dim)\n",
    "        \"\"\"\n",
    "        self._cache = x # Store input indices for backward pass\n",
    "        return self.weight[x]\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for Embedding layer.\n",
    "        grad_output: gradient from the subsequent layer, shape (B, T, embedding_dim)\n",
    "        Returns: (grad_input, [grad_weight])\n",
    "        \"\"\"\n",
    "        x = self._cache # Retrieve input indices\n",
    "        grad_weight = np.zeros_like(self.weight) # Initialize gradient for weights\n",
    "\n",
    "        # Accumulate gradients for each embedding used.\n",
    "        # This is a sparse update: only update rows corresponding to input indices.\n",
    "        # if x.ndim == 1: # Handle (T,) input case\n",
    "        #     for i, idx in enumerate(x):\n",
    "        #         grad_weight[idx] += grad_output[i]\n",
    "        # else: # Handle (B, T) input case\n",
    "        #     for b in range(x.shape[0]):\n",
    "        #         for t in range(x.shape[1]):\n",
    "        #             grad_weight[x[b, t]] += grad_output[b, t]\n",
    "\n",
    "        # Efficiently accumulate gradients using np.add.at\n",
    "        # Remove loops in Embedding.backward (lets BLAS + vectorized add handle parallelism)\n",
    "        grad_weight = np.zeros_like(self.weight)\n",
    "        if x.ndim == 1:\n",
    "            np.add.at(grad_weight, x, grad_output)\n",
    "        else:\n",
    "            np.add.at(grad_weight, x.reshape(-1), grad_output.reshape(-1, grad_output.shape[-1]))\n",
    "\n",
    "        # For embedding layer, there's no gradient to pass back to the input (it's just indices).\n",
    "        return None, [grad_weight]\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A simple Linear (fully connected) layer.\n",
    "    Performs y = x @ W + b.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        # Initialize weights with small random values\n",
    "        self.weight = np.random.randn(in_features, out_features) * 0.02\n",
    "        self.bias = np.zeros(out_features) if bias else None\n",
    "        self._parameters = [self.weight]\n",
    "        if self.bias is not None:\n",
    "            self._parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for Linear layer.\n",
    "        x: input tensor, shape (..., in_features)\n",
    "        Returns: output tensor, shape (..., out_features)\n",
    "        \"\"\"\n",
    "        self._cache = x # Store input for backward pass\n",
    "        out = x.dot(self.weight)\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for Linear layer.\n",
    "        grad_output: gradient from the subsequent layer, shape (..., out_features)\n",
    "        Returns: (grad_input, [grad_weight, grad_bias])\n",
    "        \"\"\"\n",
    "        x = self._cache # Retrieve input\n",
    "        original_x_shape = x.shape\n",
    "\n",
    "        # Reshape x and grad_output to 2D for matrix multiplication for gradients\n",
    "        # This handles arbitrary leading dimensions (B, T, ...)\n",
    "        x_reshaped = x.reshape(-1, original_x_shape[-1]) # (N, in_features)\n",
    "        grad_output_reshaped = grad_output.reshape(-1, grad_output.shape[-1]) # (N, out_features)\n",
    "\n",
    "        # Gradient for weight: dL/dW = x.T @ dL/dy\n",
    "        grad_weight = x_reshaped.T @ grad_output_reshaped\n",
    "\n",
    "        grad_bias = None\n",
    "        if self.bias is not None:\n",
    "            # Gradient for bias: dL/db = sum(dL/dy) along all dimensions except the last\n",
    "            grad_bias = np.sum(grad_output_reshaped, axis=0)\n",
    "\n",
    "        # Gradient for input: dL/dx = dL/dy @ W.T\n",
    "        grad_input = grad_output_reshaped @ self.weight.T\n",
    "        grad_input = grad_input.reshape(original_x_shape) # Reshape back to original input shape\n",
    "\n",
    "        param_grads = [grad_weight]\n",
    "        if grad_bias is not None:\n",
    "            param_grads.append(grad_bias)\n",
    "\n",
    "        return grad_input, param_grads\n",
    "\n",
    "class LayerNorm(Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization layer.\n",
    "    Normalizes features across the last dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, dims, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = np.ones(dims)  # Learnable scaling parameter\n",
    "        self.beta = np.zeros(dims)  # Learnable shifting parameter\n",
    "        self._parameters = [self.gamma, self.beta]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for LayerNorm.\n",
    "        x: input tensor, shape (..., dims)\n",
    "        Returns: normalized tensor, same shape as x\n",
    "        \"\"\"\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        var = x.var(axis=-1, keepdims=True)\n",
    "        std = np.sqrt(var + self.eps)\n",
    "        x_norm = (x - mean) / std # Normalized input\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "\n",
    "        # Store intermediate values for backward pass\n",
    "        self._cache = (x, mean, std, x_norm)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for LayerNorm.\n",
    "        grad_output: gradient from subsequent layer, same shape as forward output.\n",
    "        Returns: (grad_input, [grad_gamma, grad_beta])\n",
    "        \"\"\"\n",
    "        x, mean, std, x_norm = self._cache\n",
    "\n",
    "        # Gradients for gamma and beta (sum over all dimensions except the last)\n",
    "        grad_gamma = np.sum(grad_output * x_norm, axis=tuple(range(grad_output.ndim - 1)))\n",
    "        grad_beta = np.sum(grad_output, axis=tuple(range(grad_output.ndim - 1)))\n",
    "\n",
    "        # Gradient for x_norm (before gamma/beta scaling)\n",
    "        grad_x_norm = grad_output * self.gamma\n",
    "\n",
    "        # Gradient for x (through normalization formula)\n",
    "        # This is a common and numerically stable way to compute LayerNorm's grad_x\n",
    "        N = x.shape[-1] # Number of features in the last dimension\n",
    "\n",
    "        # Part 1: Gradient through (x - mean) / std\n",
    "        grad_x = grad_x_norm / std\n",
    "\n",
    "        # Part 2: Gradient through std (which comes from var)\n",
    "        grad_var = np.sum(grad_x_norm * (x - mean) * (-0.5) * (std**(-3)), axis=-1, keepdims=True)\n",
    "\n",
    "        # Part 3: Gradient through mean\n",
    "        grad_mean = np.sum(grad_x_norm * (-1 / std), axis=-1, keepdims=True) + grad_var * (-2 / N) * np.sum(x - mean, axis=-1, keepdims=True)\n",
    "\n",
    "        grad_x += (2 / N) * grad_var * (x - mean)\n",
    "        grad_x += (1 / N) * grad_mean\n",
    "\n",
    "        return grad_x, [grad_gamma, grad_beta]\n",
    "\n",
    "class Dropout(Module):\n",
    "    \"\"\"\n",
    "    Dropout layer.\n",
    "    Randomly sets a fraction of input units to zero during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p # Dropout probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for Dropout.\n",
    "        x: input tensor\n",
    "        Returns: output tensor with dropout applied (if training)\n",
    "        \"\"\"\n",
    "        if self.setting and self.p > 0:\n",
    "            # Create a mask: True for elements to keep, False for elements to drop\n",
    "            # Scale by 1/(1-p) during training (inverted dropout)\n",
    "            mask = (np.random.rand(*x.shape) >= self.p) / (1.0 - self.p)\n",
    "            self._cache = mask # Store mask for backward pass\n",
    "            return x * mask\n",
    "        self._cache = None # No mask if not training or p=0\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for Dropout.\n",
    "        grad_output: gradient from subsequent layer.\n",
    "        Returns: (grad_input, []) - no parameters to update.\n",
    "        \"\"\"\n",
    "        if self._cache is not None: # Only apply mask if dropout was active in forward\n",
    "            mask = self._cache\n",
    "            grad_input = grad_output * mask\n",
    "        else: # If dropout was not active, simply pass gradient through\n",
    "            grad_input = grad_output\n",
    "        return grad_input, [] # Dropout has no parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7IompsKTqYX"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Attention Mechanism\n",
    "########################\n",
    "\n",
    "class MultiHeadAttention(Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention mechanism.\n",
    "    Computes attention scores and combines information from different \"heads\".\n",
    "    \"\"\"\n",
    "    def __init__(self, n_emb, head_size, n_heads, n_ctx, dropout_p):\n",
    "        super().__init__()\n",
    "        self.n_emb = n_emb\n",
    "        self.head_size = head_size  # total projection dim (e.g. 128)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = head_size // n_heads  # dimension per head (e.g. 32)\n",
    "\n",
    "        # Linear projections for Query, Key, Value\n",
    "        self.q_proj = Linear(n_emb, head_size, bias=False)\n",
    "        self.k_proj = Linear(n_emb, head_size, bias=False)\n",
    "        self.v_proj = Linear(n_emb, head_size, bias=False)\n",
    "\n",
    "        # Output projection\n",
    "        self.c_proj = Linear(head_size, n_emb)\n",
    "\n",
    "        self.attn_dropout = Dropout(dropout_p)\n",
    "        self.resid_dropout = Dropout(dropout_p)\n",
    "\n",
    "        # Causal mask to prevent looking ahead in sequence (for decoder-only models)\n",
    "        causal_mask = np.triu(np.ones((n_ctx, n_ctx)) * -1e9, k=1)\n",
    "        self.causal_mask = causal_mask\n",
    "\n",
    "        # KV cache for inference\n",
    "        self.kv_cache = None\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the KV cache.\"\"\"\n",
    "        self.kv_cache = None\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns all parameters of the attention module.\"\"\"\n",
    "        return (self.q_proj.parameters() +\n",
    "                self.k_proj.parameters() +\n",
    "                self.v_proj.parameters() +\n",
    "                self.c_proj.parameters())\n",
    "\n",
    "    def set(self, mode=True):\n",
    "        \"\"\"Sets the attention module and its sub-modules to training/eval mode.\"\"\"\n",
    "        super().set(mode) # Call base Module set to set self.set\n",
    "        self.q_proj.set(mode)\n",
    "        self.k_proj.set(mode)\n",
    "        self.v_proj.set(mode)\n",
    "        self.c_proj.set(mode)\n",
    "        self.attn_dropout.set(mode)\n",
    "        self.resid_dropout.set(mode)\n",
    "\n",
    "        # Clear cache when switching to training mode\n",
    "        if mode:\n",
    "            self.clear_cache()\n",
    "\n",
    "    def forward(self, x, use_cache):\n",
    "        \"\"\"\n",
    "        Forward pass for MultiHeadAttention.\n",
    "        x: input tensor, shape (B, T, n_emb)\n",
    "        Returns: output tensor, shape (B, T, n_emb)\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # Project input to Q, K, V\n",
    "        Q_orig = self.q_proj.forward(x)  # (B, T, head_size)\n",
    "        K_orig = self.k_proj.forward(x)\n",
    "        V_orig = self.v_proj.forward(x)\n",
    "\n",
    "        # Helper function to split heads and transpose\n",
    "        def split_heads(z):\n",
    "            B_s, T_s, H_s = z.shape\n",
    "            z = z.reshape(B_s, T_s, self.n_heads, self.d_k)\n",
    "            return z.transpose(0, 2, 1, 3) # (B, n_heads, T, d_k)\n",
    "\n",
    "        Q = split_heads(Q_orig)\n",
    "        K_new = split_heads(K_orig)\n",
    "        V_new = split_heads(V_orig)\n",
    "\n",
    "        if use_cache and not self.setting:  # Only use cache during inference\n",
    "            if self.kv_cache is not None:\n",
    "                # Concatenate with cached K, V\n",
    "                K_cached, V_cached = self.kv_cache\n",
    "                K = np.concatenate([K_cached, K_new], axis=2)  # Concat along sequence dimension\n",
    "                V = np.concatenate([V_cached, V_new], axis=2)\n",
    "            else:\n",
    "                K = K_new\n",
    "                V = V_new\n",
    "            \n",
    "            # Update cache with new K, V\n",
    "            self.kv_cache = (K, V)\n",
    "        else:\n",
    "            # Training mode or cache disabled\n",
    "            K = K_new\n",
    "            V = V_new\n",
    "\n",
    "        # Get actual sequence length for attention computation\n",
    "        actual_seq_len = K.shape[2]\n",
    "        \n",
    "        # Compute scaled dot-product attention scores\n",
    "        # (B, n_heads, T, d_k) @ (B, n_heads, d_k, actual_seq_len) -> (B, n_heads, T, actual_seq_len)\n",
    "        scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply causal mask (prevents attending to future tokens)\n",
    "        # Adjust mask for the actual sequence lengths\n",
    "        if use_cache and T == 1 and actual_seq_len > 1:\n",
    "            # For single token generation, create a mask for the last position\n",
    "            # attending to all previous positions (including itself)\n",
    "            mask = np.zeros((1, actual_seq_len))\n",
    "            # No masking needed for single token attending to past\n",
    "        else:\n",
    "            # Normal case: create causal mask for current sequence length\n",
    "            mask = self.causal_mask[:T, :actual_seq_len]\n",
    "        \n",
    "        masked_scores = scores + mask\n",
    "\n",
    "        attn_weights = softmax(masked_scores, axis=-1)\n",
    "        attn_weights_dropped = self.attn_dropout.forward(attn_weights)\n",
    "\n",
    "        # Compute weighted sum of values\n",
    "        # (B, n_heads, T, T) @ (B, n_heads, T, d_k) -> (B, n_heads, T, d_k)\n",
    "        o = np.matmul(attn_weights_dropped, V)\n",
    "\n",
    "        # Recombine heads: transpose and reshape back to (B, T, head_size)\n",
    "        o_combined = o.transpose(0, 2, 1, 3).reshape(B, T, self.head_size)\n",
    "\n",
    "        # Final linear projection\n",
    "        out = self.c_proj.forward(o_combined)\n",
    "        out_dropped = self.resid_dropout.forward(out)\n",
    "\n",
    "        # Store all intermediate values needed for backward pass (unchanged for training)\n",
    "        if self.setting:  # Only cache for backward pass during training\n",
    "            self._cache = (x, Q_orig, K_orig, V_orig, Q, K_new, V_new, scores, masked_scores, attn_weights, attn_weights_dropped, o, o_combined)\n",
    "            \n",
    "        return out_dropped\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for MultiHeadAttention.\n",
    "        grad_output: gradient from the subsequent layer, shape (B, T, n_emb)\n",
    "        Returns: (grad_input, list_of_param_grads)\n",
    "        \"\"\"\n",
    "        (x, Q_orig, K_orig, V_orig, Q, K, V, scores, masked_scores, attn_weights, attn_weights_dropped, o, o_combined) = self._cache\n",
    "\n",
    "        # Gradients will be collected in the order of self.parameters(): q_proj, k_proj, v_proj, c_proj\n",
    "        current_mha_param_grads = []\n",
    "\n",
    "        # 1. Backward through resid_dropout\n",
    "        grad_out_dropped, _ = self.resid_dropout.backward(grad_output) # Dropout has no params\n",
    "\n",
    "        # 2. Backward through c_proj (output linear layer)\n",
    "        grad_o_combined, c_proj_grads = self.c_proj.backward(grad_out_dropped)\n",
    "\n",
    "        # 3. Undo reshape/transpose for o_combined to get grad_o\n",
    "        # grad_o_combined: (B, T, head_size)\n",
    "        # grad_o: (B, n_heads, T, d_k)\n",
    "        B, T, H = grad_o_combined.shape\n",
    "        grad_o = grad_o_combined.reshape(B, T, self.n_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "\n",
    "        # 4. Backward through matmul(attn_weights_dropped, V)\n",
    "        # o = A @ V  => dL/dA = dL/do @ V.T, dL/dV = A.T @ dL/do\n",
    "        grad_attn_weights_dropped = np.matmul(grad_o, V.transpose(0, 1, 3, 2))\n",
    "        grad_V = np.matmul(attn_weights_dropped.transpose(0, 1, 3, 2), grad_o)\n",
    "\n",
    "        # 5. Backward through attn_dropout\n",
    "        grad_attn_weights, _ = self.attn_dropout.backward(grad_attn_weights_dropped)\n",
    "\n",
    "        # 6. Backward through softmax (attn_weights = softmax(masked_scores))\n",
    "        # dL/dx = y * (dL/dy - sum(dL/dy * y)) where y = softmax(x)\n",
    "        grad_masked_scores = grad_attn_weights * attn_weights - np.sum(grad_attn_weights * attn_weights, axis=-1, keepdims=True) * attn_weights\n",
    "\n",
    "        # 7. Backward through scores + causal_mask (causal_mask is constant, so its gradient is 0)\n",
    "        grad_scores = grad_masked_scores\n",
    "\n",
    "        # 8. Backward through scaled dot-product: scores = (Q @ K.T) / sqrt(d_k)\n",
    "        # Let S = Q @ K.T / sqrt(d_k)\n",
    "        # dL/dQ = (dL/dS @ K) / sqrt(d_k)\n",
    "        # dL/dK = (dL/dS.T @ Q) / sqrt(d_k)\n",
    "        grad_Q = np.matmul(grad_scores, K) / math.sqrt(self.d_k)\n",
    "        grad_K = np.matmul(grad_scores.transpose(0, 1, 3, 2), Q) / math.sqrt(self.d_k)\n",
    "\n",
    "        # 9. Undo split_heads for Q, K, V to get gradients for original Q_orig, K_orig, V_orig\n",
    "        def un_split_heads(z_grad, original_shape):\n",
    "            B_s, NH_s, T_s, DK_s = z_grad.shape\n",
    "            z_grad = z_grad.transpose(0, 2, 1, 3) # (B, T, n_heads, d_k)\n",
    "            return z_grad.reshape(original_shape) # (B, T, head_size)\n",
    "\n",
    "        grad_Q_orig = un_split_heads(grad_Q, Q_orig.shape)\n",
    "        grad_K_orig = un_split_heads(grad_K, K_orig.shape)\n",
    "        grad_V_orig = un_split_heads(grad_V, V_orig.shape)\n",
    "\n",
    "        # 10. Backward through q_proj, k_proj, v_proj\n",
    "        grad_x_q, q_proj_grads = self.q_proj.backward(grad_Q_orig)\n",
    "        grad_x_k, k_proj_grads = self.k_proj.backward(grad_K_orig)\n",
    "        grad_x_v, v_proj_grads = self.v_proj.backward(grad_V_orig)\n",
    "\n",
    "        # Sum gradients for the input 'x' from all three paths (Q, K, V)\n",
    "        grad_x = grad_x_q + grad_x_k + grad_x_v\n",
    "\n",
    "        # Assemble gradients in the correct order: q_proj, k_proj, v_proj, c_proj\n",
    "        current_mha_param_grads.extend(q_proj_grads)\n",
    "        current_mha_param_grads.extend(k_proj_grads)\n",
    "        current_mha_param_grads.extend(v_proj_grads)\n",
    "        current_mha_param_grads.extend(c_proj_grads)\n",
    "\n",
    "        return grad_x, current_mha_param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4jWRdI3TILl"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Multi-Layer Perceptron (MLP)\n",
    "########################\n",
    "\n",
    "class MLP(Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron block, typically used in Transformer after attention.\n",
    "    Consists of two linear layers with GELU activation and dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_emb, dropout_p):\n",
    "        super().__init__()\n",
    "        # First linear layer (expands dimension)\n",
    "        self.c_fc = Linear(n_emb, 4 * n_emb)\n",
    "        # Second linear layer (projects back to original dimension)\n",
    "        self.c_proj = Linear(4 * n_emb, n_emb)\n",
    "        self.dropout = Dropout(dropout_p)\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns all parameters of the MLP module.\"\"\"\n",
    "        return self.c_fc.parameters() + self.c_proj.parameters()\n",
    "\n",
    "    def set(self, mode=True):\n",
    "        \"\"\"Sets the MLP module and its sub-modules to training/eval mode.\"\"\"\n",
    "        super().set(mode)\n",
    "        self.c_fc.set(mode)\n",
    "        self.c_proj.set(mode)\n",
    "        self.dropout.set(mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for MLP.\n",
    "        x: input tensor, shape (B, T, n_emb)\n",
    "        Returns: output tensor, shape (B, T, n_emb)\n",
    "        \"\"\"\n",
    "        self._cache_x = x # Store input to MLP for backward pass\n",
    "\n",
    "        fc_out = self.c_fc.forward(x)\n",
    "        gelu_out = gelu(fc_out)\n",
    "        proj_out = self.c_proj.forward(gelu_out)\n",
    "        dropped_out = self.dropout.forward(proj_out)\n",
    "\n",
    "        # Store intermediate results for backward pass\n",
    "        self._cache = (fc_out, gelu_out, proj_out)\n",
    "        return dropped_out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for MLP.\n",
    "        grad_output: gradient from subsequent layer.\n",
    "        Returns: (grad_input, list_of_param_grads)\n",
    "        \"\"\"\n",
    "        x = self._cache_x\n",
    "        fc_out, gelu_out, proj_out = self._cache\n",
    "\n",
    "        # Gradients will be collected in the order of self.parameters(): c_fc, c_proj\n",
    "        current_mlp_param_grads = []\n",
    "\n",
    "        # 1. Backward through dropout\n",
    "        grad_proj_out, _ = self.dropout.backward(grad_output)\n",
    "\n",
    "        # 2. Backward through c_proj\n",
    "        grad_gelu_out, c_proj_grads = self.c_proj.backward(grad_proj_out)\n",
    "\n",
    "\n",
    "        # 3. Backward through GELU activation\n",
    "        # dL/dx = dL/dy * gelu_prime(x)\n",
    "        grad_fc_out = grad_gelu_out * gelu_prime(fc_out)\n",
    "\n",
    "        # 4. Backward through c_fc\n",
    "        grad_x, c_fc_grads = self.c_fc.backward(grad_fc_out)\n",
    "\n",
    "        # Assemble gradients in the correct order: c_fc, c_proj\n",
    "        current_mlp_param_grads.extend(c_fc_grads)\n",
    "        current_mlp_param_grads.extend(c_proj_grads)\n",
    "\n",
    "        return grad_x, current_mlp_param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTTLhgQxTRXL"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Transformer Block\n",
    "########################\n",
    "\n",
    "class Block(Module):\n",
    "    \"\"\"\n",
    "    A single Transformer Block.\n",
    "    Consists of LayerNorm, MultiHeadAttention, another LayerNorm, and an MLP.\n",
    "    Includes residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_emb, head_size, n_heads, n_ctx, dropout_p):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(n_emb)\n",
    "        self.mha = MultiHeadAttention(n_emb, head_size, n_heads, n_ctx, dropout_p)\n",
    "        self.ln_2 = LayerNorm(n_emb)\n",
    "        self.mlp = MLP(n_emb, dropout_p)\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns all parameters of the Transformer Block.\"\"\"\n",
    "        return (self.ln_1.parameters() +\n",
    "                self.mha.parameters() + # Order changed for consistency\n",
    "                self.ln_2.parameters() +\n",
    "                self.mlp.parameters())\n",
    "\n",
    "    def set(self, mode=True):\n",
    "        \"\"\"Sets the block and its sub-modules to training/eval mode.\"\"\"\n",
    "        super().set(mode)\n",
    "        self.ln_1.set(mode)\n",
    "        self.ln_2.set(mode)\n",
    "        self.mha.set(mode)\n",
    "        self.mlp.set(mode)\n",
    "\n",
    "    def forward(self, x, use_cache):\n",
    "        \"\"\"\n",
    "        Forward pass for a Transformer Block.\n",
    "        x: input tensor, shape (B, T, n_emb)\n",
    "        Returns: output tensor, shape (B, T, n_emb)\n",
    "        \"\"\"\n",
    "        self._cache_x = x # Store input for the first residual connection\n",
    "\n",
    "        # First residual connection: x + MHA(LayerNorm(x))\n",
    "        ln1_out = self.ln_1.forward(x)\n",
    "        mha_out = self.mha.forward(ln1_out, use_cache)\n",
    "        x_res1 = x + mha_out # Residual connection 1\n",
    "\n",
    "        # Second residual connection: x_res1 + MLP(LayerNorm(x_res1))\n",
    "        ln2_out = self.ln_2.forward(x_res1)\n",
    "        mlp_out = self.mlp.forward(ln2_out)\n",
    "        out = x_res1 + mlp_out # Residual connection 2\n",
    "\n",
    "        # Store intermediate values for backward pass\n",
    "        self._cache = (ln1_out, mha_out, ln2_out, mlp_out, x_res1)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for a Transformer Block.\n",
    "        grad_output: gradient from subsequent layer.\n",
    "        Returns: (grad_input, list_of_param_grads)\n",
    "        \"\"\"\n",
    "        x = self._cache_x\n",
    "        ln1_out, mha_out, ln2_out, mlp_out, x_res1 = self._cache\n",
    "\n",
    "        # Gradients will be collected in the order of self.parameters(): ln_1, mha, ln_2, mlp\n",
    "        current_block_param_grads = []\n",
    "\n",
    "        # 1. Backward through second residual connection and MLP\n",
    "        grad_x_res1_from_res2 = grad_output\n",
    "        grad_mlp_out = grad_output\n",
    "\n",
    "        grad_ln2_out, mlp_grads = self.mlp.backward(grad_mlp_out)\n",
    "\n",
    "        grad_x_res1_from_ln2, ln2_grads = self.ln_2.backward(grad_ln2_out)\n",
    "\n",
    "        # Sum gradients for x_res1 from both paths\n",
    "        grad_x_res1 = grad_x_res1_from_res2 + grad_x_res1_from_ln2\n",
    "\n",
    "        # 2. Backward through first residual connection and MHA\n",
    "        grad_x_from_res1 = grad_x_res1\n",
    "        grad_mha_out = grad_x_res1\n",
    "\n",
    "        grad_ln1_out, mha_grads = self.mha.backward(grad_mha_out)\n",
    "\n",
    "        grad_x_from_ln1, ln1_grads = self.ln_1.backward(grad_ln1_out)\n",
    "\n",
    "        # Sum gradients for the initial input 'x' from both paths\n",
    "        grad_x = grad_x_from_res1 + grad_x_from_ln1\n",
    "\n",
    "        # Assemble gradients in the correct order: ln_1, mha, ln_2, mlp\n",
    "        current_block_param_grads.extend(ln1_grads)\n",
    "        current_block_param_grads.extend(mha_grads)\n",
    "        current_block_param_grads.extend(ln2_grads)\n",
    "        current_block_param_grads.extend(mlp_grads)\n",
    "\n",
    "        return grad_x, current_block_param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f__OAAbdTB3n"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Optimizer\n",
    "########################\n",
    "\n",
    "class AdamW:\n",
    "    \"\"\"\n",
    "    AdamW optimizer implementation.\n",
    "    Includes adaptive learning rates and weight decay.\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters, learning_rate=1e-3, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01):\n",
    "        self.params = parameters # List of all trainable parameters (NumPy arrays)\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.t = 0 # Timestep counter\n",
    "\n",
    "        # Initialize first and second moment estimates for each parameter\n",
    "        self.m = {id(p): np.zeros_like(p) for p in self.params}\n",
    "        self.v = {id(p): np.zeros_like(p) for p in self.params}\n",
    "\n",
    "    def step(self, grads):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step (parameter update).\n",
    "        grads: a list of gradients, corresponding to self.params.\n",
    "        \"\"\"\n",
    "        self.t += 1 # Increment timestep\n",
    "        for p, g in zip(self.params, grads):\n",
    "            pid = id(p) # Use object ID for unique parameter identification\n",
    "\n",
    "            # Skip weight decay for 1D params (biases, LayerNorm gamma/beta)\n",
    "            if self.weight_decay and p.ndim > 1:\n",
    "                # Apply weight decay (L2 regularization)\n",
    "                # This is applied directly to the gradient before the Adam update\n",
    "                g = g + self.weight_decay * p\n",
    "\n",
    "            # Update biased first moment estimate\n",
    "            self.m[pid] = self.beta1 * self.m[pid] + (1 - self.beta1) * g\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[pid] = self.beta2 * self.v[pid] + (1 - self.beta2) * (g * g)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = self.m[pid] / (1 - self.beta1 ** self.t)\n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            v_hat = self.v[pid] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update parameters\n",
    "            p -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwZhSm2QUmj7"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Loss Function\n",
    "########################\n",
    "\n",
    "def cross_entropy_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss and its gradient with respect to logits.\n",
    "    logits: (B, T, vocab_size) - raw predictions from the model\n",
    "    targets: (B, T) - true token IDs\n",
    "    Returns: (loss_value, grad_logits)\n",
    "    \"\"\"\n",
    "    B, T, C = logits.shape\n",
    "    logits_flat = logits.reshape(B * T, C)\n",
    "    targets_flat = targets.reshape(B * T)\n",
    "\n",
    "    # For numerical stability: subtract max logit from all logits before exponentiation\n",
    "    logits_max = np.max(logits_flat, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits_flat - logits_max)\n",
    "\n",
    "    sum_exp_logits = np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    probs = exp_logits / sum_exp_logits # Softmax probabilities\n",
    "\n",
    "    # Compute loss: - sum(target_one_hot * log(probs))\n",
    "    log_probs = np.log(probs + 1e-9) # Add epsilon for numerical stability to avoid log(0)\n",
    "    loss = -np.mean(log_probs[np.arange(B * T), targets_flat])\n",
    "\n",
    "    # Compute gradient of cross-entropy loss with respect to logits\n",
    "    # The derivative of Cross-Entropy + Softmax is (probs - one_hot_targets)\n",
    "    one_hot_targets = np.zeros_like(probs)\n",
    "    one_hot_targets[np.arange(B * T), targets_flat] = 1\n",
    "\n",
    "    grad_logits = probs - one_hot_targets # Shape: (B*T, C)\n",
    "    grad_logits = grad_logits.reshape(B, T, C) # Reshape back to (B, T, C)\n",
    "\n",
    "    return loss, grad_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fLVEmUnUtAC"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Calculate Value and Grand\n",
    "########################\n",
    "\n",
    "def value_and_grad(model, x, y):\n",
    "    \"\"\"\n",
    "    Performs a forward pass to compute loss and then a backward pass\n",
    "    to compute gradients for all model parameters.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    logits = model.forward(x, use_cache=False)\n",
    "    # Compute loss and get initial gradient for logits from the loss function\n",
    "    loss, grad_logits = cross_entropy_loss(logits, y)\n",
    "\n",
    "    # Backward pass: The model's backward method takes the gradient from the loss\n",
    "    # and propagates it back through all layers, returning gradients for parameters.\n",
    "    _, grads = model.backward(grad_logits) # grad_input for model is None\n",
    "\n",
    "    return loss, grads\n",
    "\n",
    "def value_and_nograd(model, x, y):\n",
    "    \"\"\"\n",
    "    Performs a forward pass to compute loss\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    logits = model.forward(x, use_cache=False)\n",
    "    loss, _ = cross_entropy_loss(logits, y) \n",
    "\n",
    "    return loss, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3U5D8UeTR7r"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# GPT Model Definition\n",
    "########################\n",
    "\n",
    "class GPT(Module):\n",
    "    \"\"\"\n",
    "    A minimal GPT (Generative Pre-trained Transformer) model.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_ctx, n_emb, n_layers, head_size, n_heads, dropout_p):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.vocab_size = 1 # Placeholder for vocabulary size, should updated\n",
    "        self.wte = Embedding(self.vocab_size, n_emb)    # Token embeddings\n",
    "        self.wpe = Embedding(n_ctx, n_emb)        # Positional embeddings\n",
    "        self.blocks = [Block(n_emb, head_size, n_heads, n_ctx, dropout_p)\n",
    "                       for _ in range(n_layers)]    # Stack of Transformer blocks\n",
    "        self.ln_f = LayerNorm(n_emb)                # Final Layer Normalization\n",
    "        self.lm_head = Linear(n_emb, self.vocab_size)    # Language modeling head (output logits)\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns all parameters of the GPT model.\"\"\"\n",
    "        params = []\n",
    "        params += self.wte.parameters()\n",
    "        params += self.wpe.parameters()\n",
    "        for block in self.blocks:\n",
    "            params += block.parameters()\n",
    "        params += self.ln_f.parameters()\n",
    "        params += self.lm_head.parameters()\n",
    "        return params\n",
    "\n",
    "    def set(self, mode=True):\n",
    "        \"\"\"Sets the GPT model and all its sub-modules to training/eval mode.\"\"\"\n",
    "        super().set(mode) # Call base Module train to set self.set\n",
    "        self.wte.set(mode)\n",
    "        self.wpe.set(mode)\n",
    "        for block in self.blocks:\n",
    "            block.set(mode)\n",
    "        self.ln_f.set(mode)\n",
    "        self.lm_head.set(mode)\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear all KV caches in the model.\"\"\"\n",
    "        for block in self.blocks:\n",
    "            block.mha.clear_cache()\n",
    "        if hasattr(self, '_position_offset'):\n",
    "            del self._position_offset\n",
    "\n",
    "    def forward(self, x, use_cache):\n",
    "        \"\"\"\n",
    "        Forward pass for the GPT model.\n",
    "        x: input token IDs, shape (B, T)\n",
    "        Returns: logits, shape (B, T, vocab_size)\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.wte.forward(x)   # (B, T, n_emb)\n",
    "        \n",
    "        # For KV cache, we need to offset position indices\n",
    "        if use_cache and hasattr(self, '_position_offset'):\n",
    "            # Calculate starting position with modulo wrap\n",
    "            pos_start = self._position_offset % self.n_ctx\n",
    "            pos_indices = np.arange(T) + pos_start\n",
    "            pos_indices = pos_indices % self.n_ctx  # Wrap around if needed\n",
    "        else:\n",
    "            pos_indices = np.arange(T) % self.n_ctx # Wrap around if needed\n",
    "\n",
    "        pos_emb = self.wpe.forward(pos_indices) # (T, n_emb)\n",
    "        \n",
    "        # Combine token and position embeddings (positional embeddings are broadcasted)\n",
    "        x_combined_emb = tok_emb + pos_emb\n",
    "\n",
    "        # Pass through Transformer blocks\n",
    "        current_x = x_combined_emb\n",
    "\n",
    "        # We need to store the output of each block to correctly backpropagate through the sequential blocks.\n",
    "        # However, the Block's backward method only needs its *own* input gradient, not the full history.\n",
    "        # The chain rule handles this sequentially.\n",
    "        for block in self.blocks:\n",
    "            current_x = block.forward(current_x, use_cache)\n",
    "\n",
    "        ln_f_out = self.ln_f.forward(current_x)\n",
    "        logits = self.lm_head.forward(ln_f_out)  # (B, T, vocab_size)\n",
    "\n",
    "        # Update position offset for next iteration\n",
    "        if use_cache:\n",
    "            if not hasattr(self, '_position_offset'):\n",
    "                self._position_offset = T\n",
    "            else:\n",
    "                self._position_offset += T\n",
    "\n",
    "        # Store intermediate values for backward pass\n",
    "        self._cache = (x_combined_emb, current_x, ln_f_out)\n",
    "        return logits\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for the GPT model.\n",
    "        grad_output: gradient from the loss function, shape (B, T, vocab_size)\n",
    "        Returns: (None, list_of_param_grads) - no grad_input for the whole model.\n",
    "        \"\"\"\n",
    "        (x_combined_emb, current_x_before_lnf, ln_f_out) = self._cache\n",
    "\n",
    "        # Initialize an empty list to store gradients in the correct order\n",
    "        # The order must match self.parameters()\n",
    "        ordered_param_grads = []\n",
    "\n",
    "        # 1. Backward through lm_head\n",
    "        grad_ln_f_out, lm_head_grads = self.lm_head.backward(grad_output)\n",
    "\n",
    "        # 2. Backward through ln_f (final LayerNorm)\n",
    "        grad_current_x_before_lnf, ln_f_grads = self.ln_f.backward(grad_ln_f_out)\n",
    "\n",
    "        # 3. Backward through blocks in reverse order\n",
    "        # Need to store block gradients temporarily in correct order (forward pass order)\n",
    "        # to match how self.blocks are added in parameters()\n",
    "        block_grads_temp = [None] * len(self.blocks) # Temporary storage for block gradients\n",
    "        grad_for_prev_block = grad_current_x_before_lnf\n",
    "        for i in reversed(range(len(self.blocks))):\n",
    "            block = self.blocks[i]\n",
    "            grad_for_prev_block, current_block_grads = block.backward(grad_for_prev_block)\n",
    "            block_grads_temp[i] = current_block_grads # Store in forward order index\n",
    "\n",
    "        # 4. Backward through token + position embeddings addition\n",
    "        # grad_for_prev_block is now the gradient for x_combined_emb (tok_emb + pos_emb)\n",
    "        grad_tok_emb = grad_for_prev_block # Gradient for token embeddings\n",
    "        # For position embeddings, sum gradients over the batch dimension\n",
    "        grad_pos_emb = np.sum(grad_for_prev_block, axis=0)\n",
    "\n",
    "        # 5. Backward through wte (token embeddings) and wpe (position embeddings)\n",
    "        # Embedding.backward returns (None, [grad_weight])\n",
    "        _, wte_grads = self.wte.backward(grad_tok_emb)\n",
    "        _, wpe_grads = self.wpe.backward(grad_pos_emb)\n",
    "\n",
    "        # Now, assemble the ordered_param_grads list in the same order as self.parameters()\n",
    "        ordered_param_grads.extend(wte_grads)\n",
    "        ordered_param_grads.extend(wpe_grads)\n",
    "        for grads_list_for_block in block_grads_temp: # These are already in forward order\n",
    "            ordered_param_grads.extend(grads_list_for_block)\n",
    "        ordered_param_grads.extend(ln_f_grads)\n",
    "        ordered_param_grads.extend(lm_head_grads)\n",
    "\n",
    "        return None, ordered_param_grads # No grad_input for the entire model\n",
    "\n",
    "    def generate(self, stoi, prompt, use_cache, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generates new tokens based on the model's learned probabilities.\n",
    "        prompt_ids: input sequence of token IDs to start generation.\n",
    "        max_new_tokens: maximum number of tokens to generate.\n",
    "        Returns: generated sequence of token IDs.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode for generation (disables dropout)\n",
    "        self.eval()\n",
    "\n",
    "        # Clear any existing cache\n",
    "        self.clear_cache()\n",
    "        \n",
    "        # Start with a starting token (here we use index 0, assuming it's a valid token)\n",
    "        # This could be a special <SOS> token in a more robust implementation.\n",
    "        if prompt is None:\n",
    "            ctx = np.zeros((1, 1), dtype=np.int32) # Initial context: a single token\n",
    "        else:\n",
    "            encode_ = lambda s: np.array([stoi[c] for c in s]).reshape(1, -1)\n",
    "            prompt_ids = encode_(prompt)  # Your tokenizer should return shape (1, prompt_length)\n",
    "            ctx = prompt_ids # Initial context: sequence of tokens\n",
    "\n",
    "        # Process initial prompt (if any) without cache to establish context\n",
    "        if ctx.shape[1] > 0:\n",
    "            input_seq = ctx[:, -self.n_ctx:]\n",
    "            _ = self.forward(input_seq, use_cache)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            if use_cache and ctx.shape[1] > 1:\n",
    "                # For cached generation, only process the last token\n",
    "                input_seq = ctx[:, -1:]\n",
    "            else:\n",
    "                # Use the last self.n_ctx tokens as input (or all if shorter)\n",
    "                input_seq = ctx[:, -self.n_ctx:]\n",
    "\n",
    "            logits = self.forward(input_seq, use_cache)\n",
    "\n",
    "            # Get logits for the last token in the sequence (the one to predict)\n",
    "            logits = logits[:, -1, :] # Shape: (1, vocab_size)\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probs = softmax(logits, axis=-1).flatten() # Flatten to (vocab_size,)\n",
    "\n",
    "            # Sample the next token based on probabilities\n",
    "            next_tok = np.random.choice(np.arange(probs.shape[0]), p=probs)\n",
    "\n",
    "            # Append the new token to the context\n",
    "            ctx = np.concatenate([ctx, np.array([[next_tok]], dtype=np.int32)], axis=1)\n",
    "\n",
    "        return ctx\n",
    "\n",
    "    def expand_embeddings(self, new_vocab_size):\n",
    "        \"\"\" Expands the token embeddings and output layer to accommodate a new vocabulary size. \"\"\"\n",
    "\n",
    "        # old_vocab_size is set to 1 for initial training\n",
    "        # old_vocab_size could be smaller than new_vocab_size for fine tuning\n",
    "        old_vocab_size = self.vocab_size\n",
    "        \n",
    "        if new_vocab_size > old_vocab_size:\n",
    "            print(f\"Expanding model embeddings from {old_vocab_size} to {new_vocab_size}.\")\n",
    "            # Expand token embeddings\n",
    "            self.vocab_size = new_vocab_size\n",
    "            new_wte_weight = np.random.randn(new_vocab_size, self.wte.weight.shape[1]) * 0.02\n",
    "            new_wte_weight[:old_vocab_size] = self.wte.weight\n",
    "            self.wte.weight = new_wte_weight\n",
    "            \n",
    "            # Expand output layer\n",
    "            new_lm_head_weight = np.random.randn(self.lm_head.weight.shape[0], new_vocab_size) * 0.02\n",
    "            new_lm_head_weight[:, :old_vocab_size] = self.lm_head.weight\n",
    "            self.lm_head.weight = new_lm_head_weight\n",
    "            \n",
    "            if self.lm_head.bias is not None:\n",
    "                new_bias = np.zeros(new_vocab_size)\n",
    "                new_bias[:old_vocab_size] = self.lm_head.bias\n",
    "                self.lm_head.bias = new_bias\n",
    "\n",
    "            self.wte._parameters = [self.wte.weight]\n",
    "            self.lm_head._parameters = [self.lm_head.weight]\n",
    "            if self.lm_head.bias is not None:\n",
    "                self.lm_head._parameters.append(self.lm_head.bias)\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        \"\"\" Load model weights from a JSON file. \"\"\"\n",
    "\n",
    "        print(\"--- Start loading ---\")\n",
    "        json_weights_dict = from_file(model_path, \"json\")\n",
    "\n",
    "        # Convert lists back to numpy arrays\n",
    "        weights_dict = {}\n",
    "        for key, value in json_weights_dict.items():\n",
    "            if value is not None and isinstance(value, list):\n",
    "                weights_dict[key] = np.array(value, dtype=np.float32)\n",
    "            else:\n",
    "                weights_dict[key] = value\n",
    "\n",
    "        # Update vocab_size from loaded weights BEFORE restoring weights\n",
    "        self.vocab_size = weights_dict['wte_weight'].shape[0]\n",
    "\n",
    "        # Restore weights to the model\n",
    "        self.wte.weight = weights_dict['wte_weight']\n",
    "        self.wpe.weight = weights_dict['wpe_weight']\n",
    "        self.ln_f.gamma = weights_dict['ln_f_gamma']\n",
    "        self.ln_f.beta = weights_dict['ln_f_beta']\n",
    "        self.lm_head.weight = weights_dict['lm_head_weight']\n",
    "        if weights_dict['lm_head_bias'] is not None:\n",
    "            self.lm_head.bias = weights_dict['lm_head_bias']\n",
    "\n",
    "        # UPDATE the main model's top-level _parameters\n",
    "        self.wte._parameters = [self.wte.weight]\n",
    "        self.wpe._parameters = [self.wpe.weight]\n",
    "        self.ln_f._parameters = [self.ln_f.gamma, self.ln_f.beta]\n",
    "        self.lm_head._parameters = [self.lm_head.weight]\n",
    "        if self.lm_head.bias is not None:\n",
    "            self.lm_head._parameters.append(self.lm_head.bias)\n",
    "        \n",
    "        # Restore block weights\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            # Layer norms\n",
    "            block.ln_1.gamma = weights_dict[f'block_{i}_ln1_gamma']\n",
    "            block.ln_1.beta = weights_dict[f'block_{i}_ln1_beta']\n",
    "            block.ln_2.gamma = weights_dict[f'block_{i}_ln2_gamma']\n",
    "            block.ln_2.beta = weights_dict[f'block_{i}_ln2_beta']\n",
    "\n",
    "            # UPDATE LayerNorm _parameters\n",
    "            block.ln_1._parameters = [block.ln_1.gamma, block.ln_1.beta]\n",
    "            block.ln_2._parameters = [block.ln_2.gamma, block.ln_2.beta]\n",
    "            \n",
    "            # Multi-head attention\n",
    "            block.mha.q_proj.weight = weights_dict[f'block_{i}_mha_q_weight']\n",
    "            block.mha.k_proj.weight = weights_dict[f'block_{i}_mha_k_weight']\n",
    "            block.mha.v_proj.weight = weights_dict[f'block_{i}_mha_v_weight']\n",
    "            block.mha.c_proj.weight = weights_dict[f'block_{i}_mha_c_weight']\n",
    "            if weights_dict[f'block_{i}_mha_c_bias'] is not None:\n",
    "                block.mha.c_proj.bias = weights_dict[f'block_{i}_mha_c_bias']\n",
    "\n",
    "            # UPDATE MHA projection _parameters\n",
    "            block.mha.q_proj._parameters = [block.mha.q_proj.weight]\n",
    "            block.mha.k_proj._parameters = [block.mha.k_proj.weight]\n",
    "            block.mha.v_proj._parameters = [block.mha.v_proj.weight]\n",
    "            block.mha.c_proj._parameters = [block.mha.c_proj.weight]\n",
    "            if block.mha.c_proj.bias is not None:\n",
    "                block.mha.c_proj._parameters.append(block.mha.c_proj.bias)\n",
    "            \n",
    "            # MLP\n",
    "            block.mlp.c_fc.weight = weights_dict[f'block_{i}_mlp_fc_weight']\n",
    "            if weights_dict[f'block_{i}_mlp_fc_bias'] is not None:\n",
    "                block.mlp.c_fc.bias = weights_dict[f'block_{i}_mlp_fc_bias']\n",
    "            block.mlp.c_proj.weight = weights_dict[f'block_{i}_mlp_proj_weight']\n",
    "            if weights_dict[f'block_{i}_mlp_proj_bias'] is not None:\n",
    "                block.mlp.c_proj.bias = weights_dict[f'block_{i}_mlp_proj_bias']\n",
    "\n",
    "            # UPDATE MLP _parameters\n",
    "            block.mlp.c_fc._parameters = [block.mlp.c_fc.weight]\n",
    "            if block.mlp.c_fc.bias is not None:\n",
    "                block.mlp.c_fc._parameters.append(block.mlp.c_fc.bias)\n",
    "            block.mlp.c_proj._parameters = [block.mlp.c_proj.weight]\n",
    "            if block.mlp.c_proj.bias is not None:\n",
    "                block.mlp.c_proj._parameters.append(block.mlp.c_proj.bias)\n",
    "                \n",
    "        print(\"--- Stop loading ---\")\n",
    "\n",
    "    def save(self, model_path):     \n",
    "        \"\"\" Save model weights to a JSON file. \"\"\"\n",
    "\n",
    "        print(\"--- Start saving ---\")\n",
    "\n",
    "        # Extract all weight arrays from the model\n",
    "        weights_dict = {\n",
    "            # Token and position embeddings\n",
    "            'wte_weight': self.wte.weight,\n",
    "            'wpe_weight': self.wpe.weight,\n",
    "            \n",
    "            # Final layer norm\n",
    "            'ln_f_gamma': self.ln_f.gamma,\n",
    "            'ln_f_beta': self.ln_f.beta,\n",
    "            \n",
    "            # Language model head\n",
    "            'lm_head_weight': self.lm_head.weight,\n",
    "            'lm_head_bias': self.lm_head.bias if self.lm_head.bias is not None else None,\n",
    "        }\n",
    "        \n",
    "        # Add block weights\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            # Layer norms\n",
    "            weights_dict[f'block_{i}_ln1_gamma'] = block.ln_1.gamma\n",
    "            weights_dict[f'block_{i}_ln1_beta'] = block.ln_1.beta\n",
    "            weights_dict[f'block_{i}_ln2_gamma'] = block.ln_2.gamma\n",
    "            weights_dict[f'block_{i}_ln2_beta'] = block.ln_2.beta\n",
    "            \n",
    "            # Multi-head attention\n",
    "            weights_dict[f'block_{i}_mha_q_weight'] = block.mha.q_proj.weight\n",
    "            weights_dict[f'block_{i}_mha_k_weight'] = block.mha.k_proj.weight\n",
    "            weights_dict[f'block_{i}_mha_v_weight'] = block.mha.v_proj.weight\n",
    "            weights_dict[f'block_{i}_mha_c_weight'] = block.mha.c_proj.weight\n",
    "            weights_dict[f'block_{i}_mha_c_bias'] = block.mha.c_proj.bias if block.mha.c_proj.bias is not None else None\n",
    "            \n",
    "            # MLP\n",
    "            weights_dict[f'block_{i}_mlp_fc_weight'] = block.mlp.c_fc.weight\n",
    "            weights_dict[f'block_{i}_mlp_fc_bias'] = block.mlp.c_fc.bias if block.mlp.c_fc.bias is not None else None\n",
    "            weights_dict[f'block_{i}_mlp_proj_weight'] = block.mlp.c_proj.weight\n",
    "            weights_dict[f'block_{i}_mlp_proj_bias'] = block.mlp.c_proj.bias if block.mlp.c_proj.bias is not None else None\n",
    "        \n",
    "        # Convert numpy arrays to lists for JSON serialization\n",
    "        json_weights_dict = {}\n",
    "        for key, value in weights_dict.items():\n",
    "            if value is not None and hasattr(value, 'tolist'):\n",
    "                json_weights_dict[key] = value.tolist()\n",
    "            else:\n",
    "                json_weights_dict[key] = value\n",
    "\n",
    "        # Save weights only\n",
    "        to_file(model_path, \"json\", json_weights_dict)\n",
    "\n",
    "        print(\"--- Stop saving ---\")\n",
    "\n",
    "    def train(self, input_path, tokenizer_path, report_path, n_epochs, batch_size, lr):\n",
    "        \"\"\" Train the GPT model on the provided input data. \"\"\"\n",
    "        print(\"--- Start training ---\")\n",
    "\n",
    "        # Create tokenizer\n",
    "        tokenizer = CharTokenizer()\n",
    "        \n",
    "        # Tokenize the input data\n",
    "        train_data, val_data = tokenizer.tokenize(input_path)\n",
    "        \n",
    "        X_train, y_train = tokenizer.prepare_data(train_data, self.n_ctx)\n",
    "        X_val, y_val = tokenizer.prepare_data(val_data, self.n_ctx)\n",
    "\n",
    "        # If vocabulary expanded, resize model embeddings\n",
    "        vocab_size = tokenizer.vocab_size  # Get the size of the vocabulary from the tokenizer\n",
    "        if vocab_size > self.vocab_size: # If the vocabulary size has changed, expand embeddings\n",
    "            self.expand_embeddings(vocab_size)\n",
    "\n",
    "        params = self.parameters() # Get all trainable parameters from the model\n",
    "\n",
    "        optimizer = AdamW(params, learning_rate=lr)\n",
    "\n",
    "        batch_logs = []\n",
    "        epoch_logs = []\n",
    "        epoch_total_time = 0\n",
    "        total_time = 0\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Record start time\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            # Training phase\n",
    "            self.set(True) # Set model to training mode (enables dropout)\n",
    "\n",
    "            running_train_loss = 0\n",
    "            train_batch_cnt = 0\n",
    "            train_batch_all = 0\n",
    "\n",
    "            train_batches = list(tokenizer.get_batches(X_train, y_train, batch_size, shuffle=True))\n",
    "            train_batch_all = len(train_batches)\n",
    "            if not train_batches:\n",
    "                print(f\"Epoch {epoch:2}/{n_epochs:2} | No training data batches available. Skipping training for this epoch.\")\n",
    "                avg_train_loss = float('nan')\n",
    "            else:\n",
    "                for X_batch, y_batch in train_batches:\n",
    "                    train_batch_cnt += 1\n",
    "                    train_batch_start_time = time.time()  # Record start time\n",
    "                    # Compute loss and gradients\n",
    "                    loss, grads = value_and_grad(self, X_batch, y_batch)\n",
    "                    # Update model parameters using the optimizer\n",
    "                    optimizer.step(grads)\n",
    "                    running_train_loss += loss\n",
    "                    train_batch_stop_time = time.time()  # Record stop time\n",
    "                    train_batch_elapsed_time = train_batch_stop_time - train_batch_start_time\n",
    "                    # Print loss for the current batch\n",
    "                    batch_log = f\"Epoch {epoch:2}/{n_epochs:2} | Batch {train_batch_cnt:2}/{train_batch_all:2} | train_loss = {loss:.4f} | execution_time = {train_batch_elapsed_time:.4f}\"\n",
    "                    batch_logs.append(batch_log)\n",
    "                    print(batch_log)\n",
    "                avg_train_loss = running_train_loss / train_batch_cnt\n",
    "\n",
    "            # Validation phase\n",
    "            self.set(False) # Set model to evaluation mode (disables dropout)\n",
    "\n",
    "            running_val_loss = 0\n",
    "            val_batch_cnt = 0\n",
    "            val_batch_all = 0\n",
    "\n",
    "            val_batches = list(tokenizer.get_batches(X_val, y_val, batch_size, shuffle=False))\n",
    "            val_batch_all = len(val_batches)\n",
    "            if not val_batches:\n",
    "                print(f\"Epoch {(epoch+1):2}/{n_epochs:2} | No validation data batches available. Skipping validation for this epoch.\")\n",
    "                avg_val_loss = float('nan') # Indicate no validation was performed\n",
    "            else:\n",
    "                for X_batch, y_batch in val_batches:\n",
    "                    val_batch_cnt += 1\n",
    "                    val_batch_start_time = time.time()  # Record start time\n",
    "                    # In validation, only forward pass and loss computation are needed\n",
    "                    loss, _ = value_and_nograd(self, X_batch, y_batch)\n",
    "                    running_val_loss += loss\n",
    "                    val_batch_stop_time = time.time()  # Record stop time\n",
    "                    val_batch_elapsed_time = val_batch_stop_time - val_batch_start_time  \n",
    "                    # Print loss for the current batch\n",
    "                    batch_log = f\"Epoch {epoch:2}/{n_epochs:2} | Batch {val_batch_cnt:2}/{val_batch_all:2} | val_loss = {loss:.4f} | execution_time = {val_batch_elapsed_time:.4f}\"\n",
    "                    batch_logs.append(batch_log)\n",
    "                    print(batch_log)\n",
    "                avg_val_loss = running_val_loss / val_batch_cnt\n",
    "\n",
    "            # Record stop time\n",
    "            epoch_stop_time = time.time()\n",
    "\n",
    "            # Calculate elapsed time for the epoch\n",
    "            epoch_elapsed_time = epoch_stop_time - epoch_start_time  \n",
    "            epoch_total_time += epoch_elapsed_time\n",
    "\n",
    "            # Print average losses for the epoch\n",
    "            epoch_log = f\"Epoch {(epoch+1):2}/{n_epochs:2} | train_loss = {avg_train_loss:.4f} | val_loss = {avg_val_loss:.4f} | execution_time = {epoch_elapsed_time:.4f}\"\n",
    "            epoch_logs.append(epoch_log)\n",
    "            print(epoch_log)\n",
    "\n",
    "        epoch_total_time_avg = epoch_total_time / n_epochs\n",
    "        total_time += epoch_total_time\n",
    "        print(f\"Average epoch time: {epoch_total_time_avg:.4f}\")        \n",
    "\n",
    "        # Create the report object\n",
    "        report = {\n",
    "            \"num_epochs\": n_epochs,\n",
    "            \"train_batches_per_epoch\": train_batch_all,\n",
    "            \"val_batches_per_epoch\": val_batch_all,\n",
    "            \"average_train_loss_per_epoch\": avg_train_loss,\n",
    "            \"average_val_loss_per_epoch\": avg_val_loss,\n",
    "            \"average_time_per_epoch\": epoch_total_time_avg,\n",
    "            \"batch_logs\": batch_logs,\n",
    "            \"epoch_logs\": epoch_logs,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "\n",
    "        # Save the report to a JSON file\n",
    "        to_file(report_path, \"json\", report)\n",
    "\n",
    "        #Create the tokenizer object\n",
    "        tokenizer_json = tokenizer.to_dict()\n",
    "\n",
    "        # Save the tokenizer to a JSON file\n",
    "        to_file(tokenizer_path, \"json\", tokenizer_json)\n",
    "        \n",
    "        print(\"--- Stop training ---\")\n",
    "\n",
    "    def inference(self, prompt, kv_cache, tokenizer_path, completion_path):\n",
    "        \"\"\" Perform inference with the trained model using a given prompt. \"\"\"\n",
    "        print(\"--- Start Inference ---\")\n",
    "\n",
    "        # Create tokenizer\n",
    "        tokenizer = CharTokenizer()\n",
    "        tokenizer_dict = from_file(tokenizer_path, \"json\")\n",
    "        tokenizer.from_dict(tokenizer_dict)\n",
    "        self.vocab_size = tokenizer.vocab_size  # Update model vocabulary size from tokenizer\n",
    "        \n",
    "        # If vocabulary expanded, resize model embeddings / Is not needed as load is loaded expanded matixes\n",
    "        # vocab_size = tokenizer.vocab_size  # Get the size of the vocabulary from the tokenizer\n",
    "        # if vocab_size > self.vocab_size: # If the vocabulary size has changed, expand embeddings\n",
    "        #     self.expand_embeddings(vocab_size)\n",
    "\n",
    "        # Load the tokenizer's string-to-index mapping\n",
    "        stoi = tokenizer.stoi\n",
    "        \n",
    "        # Initialize the total inference time\n",
    "        inference_total_time = 0\n",
    "\n",
    "        # Generate 500 new tokens\n",
    "        inference_max_tokens = 500\n",
    "\n",
    "        # Record start time\n",
    "        inference_start_time = time.time()\n",
    "\n",
    "        # Generate text based on given token ids\n",
    "        generation_ids = self.generate(stoi, prompt, kv_cache, inference_max_tokens)\n",
    "        \n",
    "        # Decode the generated token IDs back to text\n",
    "        generation = tokenizer.decode(generation_ids[0].tolist())\n",
    "\n",
    "        # Record stop time\n",
    "        inference_stop_time = time.time() \n",
    "\n",
    "        # Calculate elapsed time for inference\n",
    "        inference_elapsed_time = inference_stop_time - inference_start_time  \n",
    "        inference_total_time += inference_elapsed_time\n",
    "\n",
    "        # Create a completion object with the prompt, generated text, and inference time\n",
    "        completion = { \"prompt\": prompt, \"generation\": generation, \"inference_max_tokens\": inference_max_tokens, \"inference_total_time\": inference_total_time }\n",
    "\n",
    "        print(completion)\n",
    "\n",
    "        # Save the completion text to a JSON file        \n",
    "        to_file(completion_path, \"json\", completion)\n",
    "\n",
    "        print(\"--- Stop Inference ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Runtime configuration\n",
    "########################\n",
    "\n",
    "runtime_path_inp = input(\"Enter the runtime path ('same', '<path>'): \").strip().lower()\n",
    "runtime_plan_inp = input(\"Enter the runtime plan ('train', 'finetune', 'inference'): \").strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Runtime variables\n",
    "########################\n",
    "\n",
    "if runtime_path_inp == \"same\":\n",
    "    runtime_path = \".\"\n",
    "else:\n",
    "    runtime_path = runtime_path_inp\n",
    "    \n",
    "runtime_plan = runtime_plan_inp\n",
    "\n",
    "configs_path = f\"{runtime_path}/configs\"\n",
    "tokenizers_path = f\"{runtime_path}/tokenizers\"\n",
    "inputs_path = f\"{runtime_path}/inputs\"\n",
    "outputs_path = f\"{runtime_path}/outputs\"\n",
    "models_path = f\"{runtime_path}/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Runtime execution\n",
    "########################\n",
    "\n",
    "if runtime_plan == \"train\":\n",
    "    # Generate a UUID (random UUID)\n",
    "    runtime_uuid = uuid.uuid4()\n",
    "    config_path = f\"{configs_path}/current.json\"\n",
    "    config = from_file(config_path, \"json\")\n",
    "elif runtime_plan == \"finetune\" or runtime_plan == \"inference\":\n",
    "    # Provide the UUID or Last\n",
    "    runtime_uuid_inp = input(\"Enter the model configuration ('<uuid>','last'): \").lower()\n",
    "\n",
    "    # Load the last UUID and Prompt\n",
    "    if runtime_uuid_inp == \"last\":\n",
    "        config_path = f\"{configs_path}/last.json\"   \n",
    "        last = from_file(config_path, \"json\")\n",
    "        runtime_uuid_last = last['last_uuid']            \n",
    "        prompt_last = last['last_prompt']\n",
    "        runtime_uuid = runtime_uuid_last\n",
    "    else:\n",
    "        runtime_uuid = runtime_uuid_inp\n",
    "    \n",
    "    # Load the configuration from a file\n",
    "    if is_valid_guid(runtime_uuid):\n",
    "        config_path = f\"{configs_path}/config_{runtime_uuid}.json\"\n",
    "        config = from_file(config_path, \"json\")\n",
    "        print(f\"Runtime GUID: {runtime_uuid}\")\n",
    "    else:\n",
    "        print(f\"Invalid UUID: {runtime_uuid}\")\n",
    "        exit(1)\n",
    "else:\n",
    "    print(f\"Invalid model load plan: {runtime_plan}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0oI0zhVS72_"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Runtime Flow\n",
    "########################\n",
    "\n",
    "# Instantiate the data\n",
    "input_path = f\"{inputs_path}/input.txt\"\n",
    "\n",
    "# Instantiate the additional settings\n",
    "infr_cache = False #KV Cache\n",
    "\n",
    "# Instantiate the model\n",
    "model = GPT(config[\"n_ctx\"], config[\"n_emb\"], config[\"n_layers\"], config[\"head_size\"], config[\"n_heads\"], config[\"dropout\"])\n",
    "\n",
    "if runtime_plan == \"train\":\n",
    "    # Train the model from scratch\n",
    "    n_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    lr = config[\"lr\"]\n",
    "    tokenizer_path = f\"{tokenizers_path}/tokenizer_{runtime_uuid}.json\"\n",
    "    report_path = f\"{outputs_path}/report_{runtime_uuid}.json\"\n",
    "    model.train(input_path, tokenizer_path, report_path, n_epochs, batch_size, lr)\n",
    "    # Save the trained model weights\n",
    "    model_path = f\"{models_path}/model_{runtime_uuid}.weights\"\n",
    "    model.save(model_path)\n",
    "    # Save the runtime settings text to a file\n",
    "    config_path = f\"{configs_path}/config_{runtime_uuid}.json\"\n",
    "    to_file(config_path, \"json\", config)\n",
    "    # Inference using the trained model\n",
    "    prompt = None\n",
    "    tokenizer_path = f\"{tokenizers_path}/tokenizer_{runtime_uuid}.json\"\n",
    "    completion_path = f\"{outputs_path}/completion_{runtime_uuid}.json\"\n",
    "    model.inference(prompt, infr_cache, tokenizer_path, completion_path)\n",
    "elif runtime_plan == \"finetune\":\n",
    "    # Load the pre-trained model weights\n",
    "    model_path = f\"{models_path}/model_{runtime_uuid}.weights\"\n",
    "    model.load(model_path)\n",
    "    # Fine Tune the model with the new data\n",
    "    n_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    lr = config[\"lr\"]\n",
    "    tokenizer_path = f\"{tokenizers_path}/tokenizer_{runtime_uuid}_finetuned.json\"\n",
    "    report_path = f\"{outputs_path}/report_{runtime_uuid}.json\"\n",
    "    model.train(input_path, tokenizer_path, report_path, n_epochs, batch_size, lr)\n",
    "    # Save the fine-tuned model weights\n",
    "    model_path = f\"{models_path}/model_{runtime_uuid}_finetuned.weights\"\n",
    "    model.save(model_path)\n",
    "    # Inference using the fine-tuned model\n",
    "    prompt = None\n",
    "    tokenizer_path = f\"{tokenizers_path}/tokenizer_{runtime_uuid}_finetuned.json\"\n",
    "    completion_path = f\"{outputs_path}/completion_{runtime_uuid}_finetuned.json\"\n",
    "    model.inference(prompt, infr_cache, tokenizer_path, completion_path)\n",
    "elif runtime_plan == \"inference\":\n",
    "    prompt_inpt = input(\"Enter the prompt for inference ('<prompt>', 'last', 'none'): \").lower()\n",
    "    if prompt_inpt == \"last\":\n",
    "        prompt = prompt_last\n",
    "    elif prompt_inpt == \"none\":\n",
    "        prompt = None\n",
    "    elif prompt_inpt == \"\":\n",
    "        prompt = None\n",
    "    else:\n",
    "        prompt = prompt_inpt    \n",
    "    # Load the pre-trained or fine-tuned model weights\n",
    "    model_path = f\"{models_path}/model_{runtime_uuid}.weights\"\n",
    "    model.load(model_path)\n",
    "    # Inference with the model\n",
    "    today = dt.today()\n",
    "    today_ft = today.strftime('%Y%m%d%H%M%S')\n",
    "    tokenizer_path = f\"{tokenizers_path}/tokenizer_{runtime_uuid}.json\"\n",
    "    completion_path = f\"{outputs_path}/completion_{runtime_uuid}_{today_ft}.json\"\n",
    "    model.inference(prompt, infr_cache, tokenizer_path, completion_path)\n",
    "\n",
    "# Save the last settings text to a file\n",
    "last = { \"last_uuid\": str(runtime_uuid), \"last_prompt\": str(prompt) }\n",
    "config_path = f\"{configs_path}/last.json\"\n",
    "to_file(config_path, \"json\", last)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0s/R8uzSuCuXm8ZakpFkh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
