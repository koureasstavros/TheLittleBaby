{
    "c_sequence": "pre",
    "c_attention": "moh",
    "c_network": "moe",
    "n_ctx": 32,
    "n_emb": 128,
    "dropout": 0.1,
    "head_size": 128,
    "n_heads": 16,
    "n_layers": 4,
    "num_epochs": 1,
    "batch_size": 16,
    "lr": 0.001
}